
!split
======= Model extensions =======
label{decay:generalizations}

!split
===== Extension to a variable coefficient; Forward and Backward Euler =====

!bt
\begin{equation}
u'(t) = -a(t)u(t),\quad t\in (0,T],\quad u(0)=I
label{decay:problem:a}
\end{equation}
!et


The Forward Euler scheme:

!bt
\begin{equation}
\frac{u^{n+1} - u^n}{\Delta t} = -a(t_n)u^n
\end{equation}
!et

The Backward Euler scheme:
!bt
\begin{equation}
\frac{u^{n} - u^{n-1}}{\Delta t} = -a(t_n)u^n
\end{equation}
!et

!split
===== Extension to a variable coefficient; Crank-Nicolson =====

Eevaluting $a(t_{n+\half})$ and
using an average for $u$:
!bt
\begin{equation}
\frac{u^{n+1} - u^{n}}{\Delta t} = -a(t_{n+\half})\half(u^n + u^{n+1})
\end{equation}
!et

Using an average for $a$ and $u$:
!bt
\begin{equation}
\frac{u^{n+1} - u^{n}}{\Delta t} = -\half(a(t_n)u^n + a(t_{n+1})u^{n+1})
\end{equation}
!et

!split
===== Extension to a variable coefficient; $\theta$-rule =====

The $\theta$-rule unifies the three mentioned schemes,

!bt
\begin{equation}
\frac{u^{n+1} - u^{n}}{\Delta t} = -a((1-\theta)t_n + \theta t_{n+1})((1-\theta) u^n + \theta u^{n+1})
\end{equation}
!et
or,
!bt
\begin{equation}
\frac{u^{n+1} - u^{n}}{\Delta t} = -(1-\theta) a(t_n)u^n - \theta
a(t_{n+1})u^{n+1}
\end{equation}
!et

!split
===== Extension to a variable coefficient; operator notation =====

!bt
\begin{align*}
\lbrack D^+_t u &= -au\rbrack^n,\\
\lbrack D^-_t u &= -au\rbrack^n,\\
\lbrack D_t u &= -a\overline{u}^t\rbrack^{n+\half},\\
\lbrack D_t u &= -\overline{au}^t\rbrack^{n+\half}\\
\end{align*}
!et


!split
===== Extension to a source term =====
label{decay:source}

!bt
\begin{equation}
u'(t) = -a(t)u(t) + b(t),\quad t\in (0,T],\quad u(0)=I
label{decay:problem:ab}
\end{equation}
!et

!bt
\begin{align*}
\lbrack D^+_t u &= -au + b\rbrack^n,\\
\lbrack D^-_t u &= -au + b\rbrack^n,\\
\lbrack D_t u   &= -a\overline{u}^t + b\rbrack^{n+\half},\\
\lbrack D_t u   &= \overline{-au+b}^t\rbrack^{n+\half}
\end{align*}
!et

!split
===== Implementation of the generalized model problem =====
label{decay:general}

!bt
\begin{equation}
u^{n+1} = ((1 - \Delta t(1-\theta)a^n)u^n
+ \Delta t(\theta b^{n+1} + (1-\theta)b^n))(1 + \Delta t\theta a^{n+1})^{-1}
\end{equation}
!et

Implementation where $a(t)$ and $b(t)$ are given as
Python functions (see file "`decay_vc.py`":
"${src_genz}/decay_vc.py"):

@@@CODE src-genz/decay_vc.py def solver@def test_constant

!split
===== Implementations of variable coefficients; functions =====

Plain functions:

!bc pycod
def a(t):
    return a_0 if t < tp else k*a_0

def b(t):
    return 1
!ec

!split
===== Implementations of variable coefficients; classes =====

Better implementation: class with the parameters `a0`, `tp`, and `k`
as attributes and a *special method* `__call__` for evaluating $a(t)$:

!bc pycod
class A:
    def __init__(self, a0=1, k=2):
        self.a0, self.k = a0, k

    def __call__(self, t):
        return self.a0 if t < self.tp else self.k*self.a0

a = A(a0=2, k=1)  # a behaves as a function a(t)
!ec

!split
===== Implementations of variable coefficients; lambda function =====

idx{lambda functions}

Quick writing: a one-liner *lambda function*
!bc pycod
a = lambda t: a_0 if t < tp else k*a_0
!ec

In general,
!bc pycod
f = lambda arg1, arg2, ...: expressin
!ec
is equivalent to
!bc pycod
def f(arg1, arg2, ...):
    return expression
!ec

One can use lambda functions directly in calls:
!bc pycod
u, t = solver(1, lambda t: 1, lambda t: 1, T, dt, theta)
!ec
for a problem $u'=-u+1$, $u(0)=1$.

A lambda function can appear anywhere where a variable can appear.

!split
===== Verification via trivial solutions =====
label{decay:verify:trivial}

 * Start debugging of a new code with trying a problem
   where $u=\hbox{const} \neq 0$.
 * Choose $u=C$ (a constant). Choose any $a(t)$ and set
   $b=a(t)C$ and
   $I=C$.
 * "All" numerical methods will reproduce $u=_{\hbox{const}}$
   exactly (machine precision).
 * Often $u=C$ eases debugging.
 * In this example: *any error* in the formula for $u^{n+1}$
   make $u\neq C$!

!split
===== Verification via trivial solutions; test function =====

@@@CODE src-genz/decay_vc.py fromto: def test_constant@def test_linear


!split
===== Verification via manufactured solutions =====
label{decay:MMS}

idx{method of manufactured solutions}
idx{MMS (method of manufactured solutions)}

 * Choose *any* formula for $u(t)$
 * Fit $I$, $a(t)$, and $b(t)$ in $u'=-au+b$, $u(0)=I$,
   to make the chosen formula a solution of the ODE problem
 * Then we can always have an analytical solution (!)
 * Ideal for verification: testing convergence rates
 * Called the *method of manufactured solutions* (MMS)
 * Special case: $u$ linear in $t$, because all sound numerical
   methods will reproduce a linear $u$ exactly (machine precision)
 * $u(t) = ct + d$. $u(0)=I$ means $d=I$
 * ODE implies $c = -a(t)u + b(t)$
 * Choose $a(t)$ and $c$, and set $b(t) = c + a(t)(ct + I)$
 * Any error in the formula for $u^{n+1}$ makes $u\neq ct+I$!

!split
===== Linear manufactured solution =====

$u^n = ct_n+I$ fulfills the discrete
equations!

First,
!bt
\begin{align}
\lbrack D_t^+ t\rbrack^n &= \frac{t_{n+1}-t_n}{\Delta t}=1,
label{decay:fd2:Dop:tn:fw}\\
\lbrack D_t^- t\rbrack^n &= \frac{t_{n}-t_{n-1}}{\Delta t}=1,
label{decay:fd2:Dop:tn:bw}\\
\lbrack D_t t\rbrack^n &= \frac{t_{n+\half}-t_{n-\half}}{\Delta t}=\frac{(n+\half)\Delta t - (n-\half)\Delta t}{\Delta t}=1\label{decay:fd2:Dop:tn:cn}
\end{align}
!et

Forward Euler:

!bt
\[ [D^+ u = -au + b]^n \]
!et

$a^n=a(t_n)$, $b^n=c + a(t_n)(ct_n + I)$, and $u^n=ct_n + I$
results in

!bt
\[ c = -a(t_n)(ct_n+I) + c + a(t_n)(ct_n + I) = c \]
!et

!split
===== Test function for linear manufactured solution =====

@@@CODE src-genz/decay_vc.py def test_linear@test_conv

!split
======= Computing convergence rates =======
label{decay:convrates}

Frequent assumption on the relation between the numerical error $E$ and
some discretization parameter $\Delta t$:

!bt
\begin{equation}
E = C\Delta t^r,
label{decay:E:dt}
\end{equation}
!et

 * Unknown: $C$ and $r$.
 * Goal: estimate $r$ (and $C$) from numerical experiments, by looking at consecutive pairs of $(\Delta t_i, E_i)$ and $(\Delta t_{i-1}, E_{i-1})$.

!split
===== Estimating the convergence rate $r$ =====

Perform numerical experiments: $(\Delta t_i, E_i)$, $i=0,\ldots,m-1$.
Two methods for finding $r$ (and $C$):

  o Take the logarithm of (ref{decay:E:dt}), $\ln E = r\ln \Delta t + \ln C$,
    and fit a straight line to the data points $(\Delta t_i, E_i)$,
    $i=0,\ldots,m-1$.
  o Consider two consecutive experiments, $(\Delta t_i, E_i)$ and
    $(\Delta t_{i-1}, E_{i-1})$. Dividing the equation
    $E_{i-1}=C\Delta t_{i-1}^r$ by $E_{i}=C\Delta t_{i}^r$ and solving
    for $r$ yields
!bt
\begin{equation}
r_{i-1} = \frac{\ln (E_{i-1}/E_i)}{\ln (\Delta t_{i-1}/\Delta t_i)}
label{decay:conv:rate}
\end{equation}
!et
for $i=1,=\ldots,m-1$.

Method 2 is best.

!split
===== Brief implementation =====

Compute $r_0, r_1, \ldots, r_{m-2}$ from $E_i$ and $\Delta t_i$:

!bc pycod
def compute_rates(dt_values, E_values):
    m = len(dt_values)
    r = [log(E_values[i-1]/E_values[i])/
         log(dt_values[i-1]/dt_values[i])
         for i in range(1, m, 1)]
    # Round to two decimals
    r = [round(r_, 2) for r_ in r]
    return r
!ec

!split
===== We embed the code in a real test function =====

!bc pycod
def test_convergence_rates():
    # Create a manufactured solution
    # define u_exact(t), a(t), b(t)

    dt_values = [0.1*2**(-i) for i in range(7)]
    I = u_exact(0)

    for theta in (0, 1, 0.5):
        E_values = []
        for dt in dt_values:
            u, t = solver(I=I, a=a, b=b, T=6, dt=dt, theta=theta)
            u_e = u_exact(t)
            e = u_e - u
            E = sqrt(dt*sum(e**2))
            E_values.append(E)
        r = compute_rates(dt_values, E_values)
        print 'theta=%g, r: %s' % (theta, r)
        expected_rate = 2 if theta == 0.5 else 1
        tol = 0.1
        diff = abs(expected_rate - r[-1])
        assert diff < tol
!ec

!split
===== The manufactured solution can be computed by sympy =====

We choose $\uex(t) = \sin(t)e^{-2t}$, $a(t)=t^2$, fit $b(t)=u'(t)-a(t)$:

!bc pycod
# Create a manufactured solution with sympy
import sympy as sym
t = sym.symbols('t')
u_exact = sym.sin(t)*sym.exp(-2*t)
a = t**2
b = sym.diff(u_exact, t) + a*u_exact

# Turn sympy expressions into Python function
u_exact = sym.lambdify([t], u_exact, modules='numpy')
a = sym.lambdify([t], a, modules='numpy')
b = sym.lambdify([t], b, modules='numpy')
!ec

Complete code: "`decay_vc.py`": "${src_genz}/decay_vc.py".

!split
===== Execution =====

!bc sys
Terminal> python decay_vc.py
...
theta=0, r: [1.06, 1.03, 1.01, 1.01, 1.0, 1.0]
theta=1, r: [0.94, 0.97, 0.99, 0.99, 1.0, 1.0]
theta=0.5, r: [2.0, 2.0, 2.0, 2.0, 2.0, 2.0]
!ec


!split
===== Debugging via convergence rates =====

Potential bug: missing `a` in the denominator,

!bc pycod
u[n+1] = (1 - (1-theta)*a*dt)/(1 + theta*dt)*u[n]
!ec
Running `decay_convrate.py` gives same rates.

Why? The value of $a$... ($a=1$)

0 and 1 are *bad values* in tests!

Better:
!bc sys
Terminal> python decay_convrate.py --a 2.1 --I 0.1  \
          --dt 0.5 0.25 0.1 0.05 0.025 0.01
...
Pairwise convergence rates for theta=0:
1.49 1.18 1.07 1.04 1.02

Pairwise convergence rates for theta=0.5:
-1.42 -0.22 -0.07 -0.03 -0.01

Pairwise convergence rates for theta=1:
0.21 0.12 0.06 0.03 0.01
!ec

Forward Euler works...because $\theta=0$ hides the bug.

This bug gives $r\approx 0$:

!bc pycod
u[n+1] = ((1-theta)*a*dt)/(1 + theta*dt*a)*u[n]
!ec

!split
===== Extension to systems of ODEs =====

Sample system:

!bt
\begin{align}
u' &= a u + bv\\
v' &= cu +  dv
\end{align}
!et

The Forward Euler method:

!bt
\begin{align}
u^{n+1} &= u^n + \Delta t (a u^n + b v^n)\\
v^{n+1} &= u^n + \Delta t (cu^n + dv^n)
\end{align}
!et


!split
===== The Backward Euler method gives a system of algebraic equations =====

The Backward Euler scheme:

!bt
\begin{align}
u^{n+1} &= u^n + \Delta t (a u^{n+1} + b v^{n+1})\\
v^{n+1} &= v^n + \Delta t (c u^{n+1} + d v^{n+1})
\end{align}
!et
which is a $2\times 2$ linear system:

!bt
\begin{align}
(1 - \Delta t a)u^{n+1} + bv^{n+1} &= u^n \\
c u^{n+1} + (1 - \Delta t d) v^{n+1} &= v^n
\end{align}
!et

Crank-Nicolson also gives a $2\times 2$ linear system.

!split
======= Methods for general first-order ODEs =======
label{decay:1stODEs}

!split
===== Generic form =====

The standard form for ODEs:
!bt
\begin{equation}
u' = f(u,t),\quad u(0)=I
label{decay:ode:general}
\end{equation}
!et

$u$ and $f$: scalar or vector.

Vectors in case of ODE systems:
!bt
\[ u(t) = (u^{(0)}(t),u^{(1)}(t),\ldots,u^{(m-1)}(t))   \]
!et

!bt
\begin{align*}
f(u, t) = ( & f^{(0)}(u^{(0)},\ldots,u^{(m-1)})\\
            & f^{(1)}(u^{(0)},\ldots,u^{(m-1)}),\\
            & \vdots\\
            & f^{(m-1)}(u^{(0)}(t),\ldots,u^{(m-1)}(t)))
\end{align*}
!et

!split
===== The $\theta$-rule =====

!bt
\begin{equation}
\frac{u^{n+1}-u^n}{\Delta t} = \theta f(u^{n+1},t_{n+1}) +
(1-\theta)f(u^n, t_n)
label{decay:fd2:theta}
\end{equation}
!et
Bringing the unknown $u^{n+1}$ to the left-hand side and the known terms
on the right-hand side gives

idx{implicit schemes} idx{explicit schemes} idx{theta-rule} idx{$\theta$-rule}

!bt
\begin{equation}
u^{n+1} - \Delta t \theta f(u^{n+1},t_{n+1}) =
u^n + \Delta t(1-\theta)f(u^n, t_n)
\end{equation}
!et
This is a *nonlinear* equation in $u^{n+1}$ (unless $f$ is linear in $u$)!


!split
===== Implicit 2-step backward scheme =====

idx{backward scheme, 2-step} idx{BDF2 scheme}

!bt
\[ u'(t_{n+1}) \approx \frac{3u^{n+1} - 4u^{n} + u^{n-1}}{2\Delta t}\]
!et

Scheme:
!bt
\[ u^{n+1} = \frac{4}{3}u^n - \frac{1}{3}u^{n-1} +
\frac{2}{3}\Delta t f(u^{n+1}, t_{n+1})
label{decay:fd2:bw:2step}
\]
!et
Nonlinear equation for $u^{n+1}$.

!split
===== The Leapfrog scheme =====

idx{Leapfrog scheme}

Idea:
!bt
\begin{equation}
u'(t_n)\approx \frac{u^{n+1}-u^{n-1}}{2\Delta t} = [D_{2t} u]^n
\end{equation}
!et

Scheme:

!bt
\[ [D_{2t} u = f(u,t)]^n\]
!et
or written out,
!bt
\begin{equation}
u^{n+1} = u^{n-1} + 2 \Delta t f(u^n, t_n)
label{decay:fd2:leapfrog}
\end{equation}
!et

 * Some other scheme must be used as starter ($u^1$).
 * Explicit scheme - a nonlinear $f$ (in $u$) is trivial to handle.
 * Downside: Leapfrog is always unstable after some time.

!split
===== The filtered Leapfrog scheme =====

idx{Leapfrog scheme, filtered}

After computing $u^{n+1}$, stabilize Leapfrog by
!bt
\begin{equation}
u^n\ \leftarrow\ u^n + \gamma (u^{n-1} - 2u^n + u^{n+1})
label{decay:fd2:leapfrog:filtered}
\end{equation}
!et

!split
===== 2nd-order Runge-Kutta scheme =====

idx{Heun's method}
idx{Runge-Kutta, 2nd-order scheme}

Forward-Euler + approximate Crank-Nicolson:
!bt
\begin{align}
u^* &= u^n + \Delta t f(u^n, t_n),
label{decay:fd2:RK2:s1}\\
u^{n+1} &= u^n + \Delta t \half \left( f(u^n, t_n) + f(u^*, t_{n+1})
\right)
label{decay:fd2:RK2:s2}
\end{align}
!et

!split
===== 4th-order Runge-Kutta scheme =====
label{decay:fd2:RK4}

 * The most famous and widely used ODE method
 * 4 evaluations of $f$ per time step
 * Its "derivation": "${doc_notes}/sphinx-decay/._main_decay007.html#th-order-runge-kutta-scheme" is a very good illustration of numerical thinking!


!split
===== 2nd-order Adams-Bashforth scheme =====

idx{Adams-Bashforth scheme, 2nd order}

!bt
\begin{equation}
u^{n+1} = u^n + \half\Delta t\left( 3f(u^n, t_n) - f(u^{n-1}, t_{n-1})
\right)
label{decay:fd2:AB2}
\end{equation}
!et

!split
===== 3rd-order Adams-Bashforth scheme =====
idx{Adams-Bashforth scheme, 3rd order}

!bt
\begin{equation}
u^{n+1} = u^n + \frac{1}{12}\left( 23f(u^n, t_n) - 16 f(u^{n-1},t_{n-1})
+ 5f(u^{n-2}, t_{n-2})\right)
label{decay:fd2:AB3}
\end{equation}
!et

!split
===== The Odespy software =====

"Odespy": "https://github.com/hplgit/odespy"
features simple Python implementations of the most fundamental
schemes as well as Python interfaces to several famous packages for
solving ODEs: "ODEPACK": "https://computation.llnl.gov/casc/odepack/odepack_home.html",
"Vode": "https://computation.llnl.gov/casc/odepack/odepack_home.html",
"rkc.f": "http://www.netlib.org/ode/rkc.f",
"rkf45.f": "http://www.netlib.org/ode/rkf45.f",
"Radau5": "http://www.unige.ch/~hairer/software.html", as well
as the ODE solvers in
"SciPy": "http://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.ode.html",
"SymPy": "http://docs.sympy.org/dev/modules/mpmath/calculus/odes.html", and
"odelab": "http://olivierverdier.github.com/odelab/".

Typical usage:

!bc
# Define right-hand side of ODE
def f(u, t):
    return -a*u

import odespy
import numpy as np

# Set parameters and time mesh
I = 1; a = 2; T = 6; dt = 1.0
Nt = int(round(T/dt))
t_mesh = np.linspace(0, T, Nt+1)

# Use a 4th-order Runge-Kutta method
solver = odespy.RK4(f)
solver.set_initial_condition(I)
u, t = solver.solve(t_mesh)
!ec

!split
===== Example: Runge-Kutta methods  =====


!bc pycod
solvers = [odespy.RK2(f),
           odespy.RK3(f),
           odespy.RK4(f),
           odespy.BackwardEuler(f, nonlinear_solver='Newton')]

for solver in solvers:
    solver.set_initial_condition(I)
    u, t = solver.solve(t)

# + lots of plot code...
!ec

!split
===== Plots from the experiments =====

FIGURE: [fig-genz/decay_odespy1_png.png, width=800]

The 4-th order Runge-Kutta method (`RK4`) is the method of choice!


!split
===== Example: Adaptive Runge-Kutta methods  =====

 * Adaptive methods find "optimal" locations of the mesh points
   to ensure that the error is less than a given tolerance.
 * Downside: approximate error estimation, not always optimal
   location of points.
 * "Industry standard ODE solver": Dormand-Prince 4/5-th order
   Runge-Kutta (MATLAB's famous `ode45`).


FIGURE: [fig-genz/decay_DormandPrince_adaptivity.png, width=800]
