
======= Exercises =======

===== Problem: Visualize the accuracy of finite differences =====

label{decay:analysis:exer:fd:exp:plot}
file=decay_plot_fd_error

The purpose of this exercise is to visualize the accuracy of finite difference
approximations of the derivative of a given function.
For any finite difference approximation, take the Forward Euler difference
as an example, and any specific function, take  $u=e^{-at}$,
we may introduce an error fraction

!bt
\begin{align*}
E = \frac{[D_t^+ u]^n}{u'(t_n)} &= \frac{\exp{(-a(t_n+\Delta t))} - \exp{(-at_n)}}{-a\exp{(-at_n)\Delta t}}\\
&= \frac{1}{a\Delta t}\left(1 -\exp{(-a\Delta t)}\right),
\end{align*}
!et
and view $E$ as a function of $\Delta t$. We expect that
$\lim_{\Delta t\rightarrow 0}E=1$, while $E$ may deviate significantly from
unity for large $\Delta t$. How the error depends on $\Delta t$ is best
visualized in a graph where we use a logarithmic scale for $\Delta t$,
so we can cover many orders of magnitude of that quantity. Here is
a code segment creating an array of 100 intervals, on the logarithmic
scale, ranging from $10^{-6}$ to $10^{-0.5}$ and then plotting $E$ versus
$p=a\Delta t$ with logarithmic scale on the $p$ axis:

!bc pycod
from numpy import logspace, exp
from matplotlib.pyplot import semilogx
p = logspace(-6, -0.5, 101)
y = (1-exp(-p))/p
semilogx(p, y)
!ec
Illustrate such errors for the finite difference operators $[D_t^+u]^n$
(forward), $[D_t^-u]^n$ (backward), and $[D_t u]^n$ (centered) in
the same plot.

Perform a Taylor series expansions of the error fractions and find
the leading order $r$ in the expressions of type
$1 + Cp^r + \Oof{p^{r+1}}$, where $C$ is some constant.

!bhint
To save manual calculations and learn more about symbolic computing,
make functions for the three difference operators and use `sympy`
to perform the symbolic differences, differentiation, and Taylor series
expansion. To plot a symbolic expression `E` against `p`, convert the
expression to a Python function first: `E = sympy.lamdify([p], E)`.
!ehint

!bsol
Here is Python code for the exercise:

@@@CODE exer-analysis/decay_plot_fd_error.py
The output of the Taylor polynomials reads

!bc
forward E: (exp(p) - 1)*exp(-p)/p
Taylor series: 1 - p/2 + p**2/6 + O(p**3)
backward E: (exp(p) - 1)/p
Taylor series: 1 + p/2 + p**2/6 + O(p**3)
central E: sinh(p)/p
Taylor series: 1 + p**2/6 + O(p**3)
!ec

FIGURE: [fig-analysis/decay_plot_fd_error, width=700 frac=0.9] Plot for Exercise ref{decay:analysis:exer:fd:exp:plot}.
!esol

===== Problem: Explore the $\theta$-rule for exponential growth =====

label{decay:analysis:exer:growth}
file=exponential_growth

This exercise asks you to solve the ODE $u'=-au$ with $a < 0$ such that
the ODE models exponential growth instead of exponential decay.  A
central theme is to investigate numerical artifacts and non-physical
solution behavior.

!bsubex
Set $a=-1$ and run experiments with $\theta=0, 0.5, 1$ for
various values of $\Delta t$ to uncover numerical artifacts.
Recall that the exact solution is a
monotone, growing function when $a < 0$. Oscillations or significantly
wrong growth are signs of wrong qualitative behavior.

From the experiments, select four values of $\Delta t$ that
demonstrate the kind of numerical solutions that are characteristic
for this model.

!bsol
#Does not work: @@@CODE exer-analysis/exponential_growth.do.txt envir=None fromto: The schemes are@=== Analysis

The schemes are exactly the same as in the case $a>0$.
A program solving the problem numerically is shown below.

@@@CODE exer-analysis/exponential_growth.py fromto: from numpy import@# Exercise b

# #if FORMAT == 'ipynb'
!bc pycod
dt_values = [3.0, 0.5, 0.1, 0.01]
for dt in dt_values:
    demo(dt)
!ec
# #else
We can try different $\Delta t$ values: 3, 0.5, 0.1, and 0.01.

FIGURE: [fig-analysis/exponential_growth_demo, width=800 frac=1]
# #endif
!esol
!esubex

!bsubex
Write up the amplification factor and plot it for $\theta=0,0.5,1$
together with the exact one for $a\Delta t < 0$. Use the plot to
explain the observations made in the experiments.

!bhint
Modify the "`decay_ampf_plot.py`": "${src_analysis}/decay_ampf_plot.py" code
(in the `src/analysis` directory).
!ehint

!bsol
#@@@CODE exer-analysis/exponential_growth.do.txt envir=None fromto: === Analysis@

The amplification factor is the same as when $a>0$, but here we introduce
$p=-a\Delta t>0$ since $a < 0$:

!bt
\begin{equation}
A(p) = \frac{1+(1-\theta)p}{1-\theta p}.
\end{equation}
!et
A major problem is that the denominator can be zero when $a < 0$. This
happens for $p=1/\theta$. The exact amplification factor is $\Aex = e^{p}$.

Here is code for computing and plotting the factors:

@@@CODE exer-analysis/exponential_growth.py fromto: # Exercise b@if __name

# #if FORMAT == 'ipynb'
plot_amplification_factors('FE BE CN'.split())
# #else
FIGURE: [fig-analysis/A_growth, width=700 frac=0.9]
# #endif
!esol
!esubex

===== Problem: Explore rounding errors in numerical calculus =====

label{decay:analysis:exer:rounding}
file=rounding

!bsubex
Compute the absolute values of the errors in the numerical derivative
of $e^{-t}$ at $t=\half$ for three types of finite difference
approximations: a forward difference, a backward difference, and
a centered difference, for $\Delta t = 2^{-k}$, $k=0,4, 8, 12, \ldots, 60$.
When do rounding errors destroy the accuracy?

!bsol
The following code implements the difference approximations and runs
the experiments:

@@@CODE exer-analysis/rounding.py fromto: def forward@def trapez
The output becomes

!bc
 k    forward   backward  centered
 0    2.23E-01  4.36E-01  1.06E-01
 4    1.86E-02  1.94E-02  3.95E-04
 8    1.18E-03  1.19E-03  1.54E-06
12    7.40E-05  7.40E-05  6.03E-09
16    4.63E-06  4.63E-06  2.12E-11
20    2.89E-07  2.89E-07  3.01E-12
24    1.76E-08  1.78E-08  1.19E-10
28    6.64E-09  6.64E-09  6.64E-09
32    6.64E-09  6.64E-09  6.64E-09
36    1.42E-06  1.42E-06  1.42E-06
40    3.67E-05  3.67E-05  3.67E-05
44    8.91E-04  8.91E-04  8.91E-04
48    1.28E-02  1.28E-02  1.28E-02
52    1.07E-01  1.07E-01  1.07E-01
56    6.07E-01  6.07E-01  6.07E-01
60    6.07E-01  6.07E-01  6.07E-01
!ec
We see that for $k>32$, the errors start to grow and actually
grow further with decreasing $\Delta t$. The reason is that
the algorithms subtract two almost equal numbers in the numerator, which
means significant loss of accuracy because of rounding errors, and then we
multiply this inaccurate result by a big number. The rounding errors
in this simple algorithm can be analyzed theoretically,
see Chapter 2 in cite{Gander_2015}.
!esol
!esubex

!bsubex
Compute the absolute values of the errors in the numerical
approximation of $\int_0^4 e^{-t}dt$ using the Trapezoidal
and the Midpoint integration methods. Make a table of
the errors for $n=2^k$ intervals, $k=1,3,5=ldots,21$.
Is there any impact of rounding errors?

!bhint
The Trapezoidal rule for $\int_a^bf(x)dx$ reads

!bt
\[ \int_a^bf(x)dx\approx h(\half f(a) + \half f(b) + \sum_{i=1}^{n-1}
f(a+ih)),\quad h = \frac{b-a}{n}\tp\]
!et
The Midpoint rule is

!bt
\[ \int_a^bf(x)dx\approx h\sum_{i=1}^n f(a + (i+\half)h)\tp\]
!et
!ehint

!bsol
An appropriate code is

@@@CODE exer-analysis/rounding.py fromto: def trapez@differentiation\(
The output becomes

!bc
 k    trapezoidal    midpoint
 1     3.07E-01      1.46E-01
 3     2.04E-02      1.02E-02
 5     1.28E-03      6.39E-04
 7     7.99E-05      3.99E-05
 9     4.99E-06      2.50E-06
11     3.12E-07      1.56E-07
13     1.95E-08      9.75E-09
15     1.22E-09      6.10E-10
17     7.62E-11      3.81E-11
19     4.74E-12      2.38E-12
21     3.62E-13      2.33E-13
!ec
The error decreases with increasing $n$, as expected.
Rounding errors are not of importance. The algorithm just
adds values of $f$, and these operations are not sensitive
to rounding errors.
!esol
!esubex
