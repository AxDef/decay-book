<!--
Automatically generated HTML file from DocOnce source
(https://github.com/hplgit/doconce/)
-->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/hplgit/doconce/" />
<meta name="description" content="Finite Difference Computing with Exponential Decay Models">
<meta name="keywords" content="decay ODE,exponential decay,mesh,grid,mesh function,finite differences,forward difference,finite differences forward,difference equation,discrete equation,algebraic equation,finite difference scheme,Forward Euler scheme,backward difference,finite differences backward,backward scheme, 1-step,Backward Euler scheme,Crank-Nicolson scheme,centered difference,finite differences centered,averaging arithmetic,weighted average,theta-rule,$\theta$-rule,time step,finite difference operator notation,operator notation, finite differences,directory,folder,doc strings,printf format,format string syntax (Python),plotting curves,visualizing curves,representative (mesh function),array arithmetics,array computing,vectorization,continuous function norms,norm continuous,discrete function norms,mesh function norms,norm discrete (mesh function),error norms,scalar computing,PNG plot,PDF plot,EPS plot,viewing graphics files,cropping images,array arithmetics,array computing,vectorization,stability,amplification factor,A-stable methods,L-stable methods,interactive Python,error amplification factor,error global,consistency,stability,convergence,Monte Carlo simulation,lambda functions,method of manufactured solutions,MMS (method of manufactured solutions),convergence rate,verification,implicit schemes,explicit schemes,theta-rule,$\theta$-rule,backward scheme, 2-step,BDF2 scheme,Leapfrog scheme,Leapfrog scheme, filtered,Heun's method,Runge-Kutta, 2nd-order method,Taylor-series methods (for ODEs),Adams-Bashforth scheme, 2nd-order,Adams-Bashforth scheme, 3rd order,Runge-Kutta, 4th-order method,RK4,adaptive time stepping,Dormand-Prince Runge-Kutta 4-5 method,population dynamics,logistic model,radioactive decay,chemical reactions irreversible,chemical reactions reversible,Lotka-Volterra model,predator-prey model,terminal velocity,geometric mean,averaging geometric,scaling,Kelvin-Voigt material model,viscoelasticity,refactoring,importing modules,logger,debugging,list comprehension,command-line arguments,option-value pairs (command line),command-line arguments,reading the command line,doctests,software testing doctests,unit testing,software testing nose,software testing pytest,test function,software testing test function,relative differences,doctest in test function,unit testing,software testing unit testing (class-based),Distutils,GitHub,importing modules,problem class,solver class,wrapper (code),reproducibility,replicability,Unix wildcard notation,wildcard notation (Unix),Word (Microsoft),LibreOffice,OpenOffice,Google Docs,HTML,MathJax,LaTeX,Sphinx (typesetting tool),Markdown,IPython notebooks,Jupyter notebooks,DocOnce,replicability,relative differences">

<title>Finite Difference Computing with Exponential Decay Models</title>


<link href="https://cdn.rawgit.com/hplgit/doconce/master/bundled/html_styles/style_solarized_box/css/solarized_light_code.css" rel="stylesheet" type="text/css" title="light"/>
<script src="https://cdn.rawgit.com/hplgit/doconce/master/bundled/html_styles/style_solarized_box/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>

<link href="http://thomasf.github.io/solarized-css/solarized-light.min.css" rel="stylesheet">
<style type="text/css">
h1 {color: #b58900;}  /* yellow */
/* h1 {color: #cb4b16;}  orange */
/* h1 {color: #d33682;}  magenta, the original choice of thomasf */
code { padding: 0px; background-color: inherit; }
pre {
  border: 0pt solid #93a1a1;
  box-shadow: none;
}
.alert-text-small   { font-size: 80%;  }
.alert-text-large   { font-size: 130%; }
.alert-text-normal  { font-size: 90%;  }
.alert {
  padding:8px 35px 8px 14px; margin-bottom:18px;
  text-shadow:0 1px 0 rgba(255,255,255,0.5);
  border:1px solid #93a1a1;
  border-radius: 4px;
  -webkit-border-radius: 4px;
  -moz-border-radius: 4px;
  color: #555;
  background-color: #eee8d5;
  background-position: 10px 5px;
  background-repeat: no-repeat;
  background-size: 38px;
  padding-left: 55px;
  width: 75%;
 }
.alert-block {padding-top:14px; padding-bottom:14px}
.alert-block > p, .alert-block > ul {margin-bottom:1em}
.alert li {margin-top: 1em}
.alert-block p+p {margin-top:5px}
.alert-notice { background-image: url(https://cdn.rawgit.com/hplgit/doconce/master/bundled/html_images/small_yellow_notice.png); }
.alert-summary  { background-image:url(https://cdn.rawgit.com/hplgit/doconce/master/bundled/html_images/small_yellow_summary.png); }
.alert-warning { background-image: url(https://cdn.rawgit.com/hplgit/doconce/master/bundled/html_images/small_yellow_warning.png); }
.alert-question {background-image:url(https://cdn.rawgit.com/hplgit/doconce/master/bundled/html_images/small_yellow_question.png); }

div { text-align: justify; text-justify: inter-word; }
</style>


</head>

<!-- tocinfo
{'highest level': 0,
 'sections': [('Preface', 0, 'ch:preface', 'ch:preface'),
              ('Algorithms and implementations', 0, None, '___sec1'),
              ('Finite difference methods',
               1,
               'decay:basics',
               'decay:basics'),
              ('A basic model for exponential decay',
               2,
               'decay:model',
               'decay:model'),
              ('The exact solution', 3, None, '___sec4'),
              ('A complete problem formulation', 3, None, '___sec5'),
              ('The Forward Euler scheme',
               2,
               'decay:schemes:FE',
               'decay:schemes:FE'),
              ('Step 1: Discretizing the domain', 3, None, '___sec7'),
              ('Step 2: Fulfilling the equation at discrete time points',
               3,
               None,
               '___sec8'),
              ('Step 3: Replacing derivatives by finite differences',
               3,
               None,
               '___sec9'),
              ('Step 4: Formulating a recursive algorithm',
               3,
               None,
               '___sec10'),
              ('Interpretation', 3, None, '___sec11'),
              ('Computing with the recursive formula', 3, None, '___sec12'),
              ('The Backward Euler scheme',
               2,
               'decay:schemes:BE',
               'decay:schemes:BE'),
              ('The Crank-Nicolson scheme',
               2,
               'decay:schemes:CN',
               'decay:schemes:CN'),
              ('The unifying $\\theta$-rule',
               2,
               'decay:schemes:theta',
               'decay:schemes:theta'),
              ('Constant time step', 2, None, '___sec16'),
              ('Mathematical derivation of finite difference formulas',
               2,
               'decay:fd:taylor',
               'decay:fd:taylor'),
              ('The forward difference', 3, None, '___sec18'),
              ('The backward difference', 3, None, '___sec19'),
              ('The centered difference', 3, None, '___sec20'),
              ('Compact operator notation for finite differences',
               2,
               'decay:fd:op',
               'decay:fd:op'),
              ('Implementation', 1, 'decay:impl1', 'decay:impl1'),
              ('Computer language: Python', 2, None, '___sec23'),
              ('Making a solver function', 2, 'decay:py1', 'decay:py1'),
              ('Integer division', 2, 'decay:py2', 'decay:py2'),
              ('Doc strings', 2, None, '___sec26'),
              ('Formatting numbers', 2, None, '___sec27'),
              ('Running the program', 2, None, '___sec28'),
              ('Plotting the solution', 2, None, '___sec29'),
              ('Verifying the implementation', 2, None, '___sec30'),
              ('Running a few algorithmic steps by hand',
               3,
               None,
               '___sec31'),
              ('Computing the numerical error as a mesh function',
               2,
               'decay:computing:error',
               'decay:computing:error'),
              ('Computing the norm of the error mesh function',
               2,
               'decay:computing:error:norm',
               'decay:computing:error:norm'),
              ('Scalar computing', 3, None, '___sec34'),
              ('Experiments with computing and plotting',
               2,
               None,
               '___sec35'),
              ('Combining plot files', 3, None, '___sec36'),
              ('Plotting with SciTools', 3, None, '___sec37'),
              ('Memory-saving implementation', 2, None, '___sec38'),
              ('Exercises', 1, None, '___sec39'),
              ('Exercise 1: Define a mesh function and visualize it',
               2,
               'decay:exer:meshfunc',
               'decay:exer:meshfunc'),
              ('Remarks', 3, None, '___sec41'),
              ('Problem 2: Differentiate a function',
               2,
               'decay:exer:dudt',
               'decay:exer:dudt'),
              ('Problem 3: Experiment with divisions',
               2,
               'decay:exer:intdiv',
               'decay:exer:intdiv'),
              ('Problem 4: Experiment with wrong computations',
               2,
               'decay:exer:decay1err',
               'decay:exer:decay1err'),
              ('Problem 5: Plot the error function',
               2,
               'decay:exer:plot:error',
               'decay:exer:plot:error'),
              ('Problem 6: Change formatting of numbers and debug',
               2,
               'decay:exer:inexact:output',
               'decay:exer:inexact:output'),
              ('Analysis', 0, 'decay:analysis', 'decay:analysis'),
              ('Experimental investigations', 1, None, '___sec48'),
              ('Discouraging numerical solutions', 2, None, '___sec49'),
              ('Detailed experiments', 2, None, '___sec50'),
              ('Stability', 1, None, '___sec51'),
              ('Exact numerical solution', 2, None, '___sec52'),
              ('Stability properties derived from the amplification factor',
               2,
               None,
               '___sec53'),
              ('Accuracy', 1, None, '___sec54'),
              ('Visual comparison of amplification factors',
               2,
               None,
               '___sec55'),
              ('Series expansion of amplification factors',
               2,
               None,
               '___sec56'),
              ('The ratio of numerical and exact amplification factors',
               2,
               None,
               '___sec57'),
              ('The global error at a point',
               2,
               'decay:analysis:gobal:error',
               'decay:analysis:gobal:error'),
              ('Integrated error',
               2,
               'decay:analysis:gobal:error_int',
               'decay:analysis:gobal:error_int'),
              ('Truncation error',
               2,
               'decay:analysis:trunc',
               'decay:analysis:trunc'),
              ('Consistency, stability, and convergence',
               2,
               None,
               '___sec61'),
              ('Various types of errors in a differential equation model',
               1,
               None,
               '___sec62'),
              ('Model errors', 2, None, '___sec63'),
              ('Data errors', 2, None, '___sec64'),
              ('Discretization errors', 2, None, '___sec65'),
              ('Rounding errors', 2, None, '___sec66'),
              ('Discussion of the size of various errors',
               2,
               None,
               '___sec67'),
              ('Exercises', 1, None, '___sec68'),
              ('Problem 7: Visualize the accuracy of finite differences',
               2,
               'decay:analysis:exer:fd:exp:plot',
               'decay:analysis:exer:fd:exp:plot'),
              ('Problem 8: Explore the $\\theta$-rule for exponential growth',
               2,
               'decay:analysis:exer:growth',
               'decay:analysis:exer:growth'),
              ('Problem 9: Explore rounding errors in numerical calculus',
               2,
               'decay:analysis:exer:rounding',
               'decay:analysis:exer:rounding'),
              ('Generalizations', 0, None, '___sec72'),
              ('Model extensions', 1, None, '___sec73'),
              ('Generalization: including a variable coefficient',
               2,
               None,
               '___sec74'),
              ('Generalization: including a source term',
               2,
               'decay:source',
               'decay:source'),
              ('Implementation of the generalized model problem',
               2,
               'decay:general',
               'decay:general'),
              ('Deriving the $\\theta$-rule formula', 3, None, '___sec77'),
              ('Python code', 3, None, '___sec78'),
              ('Coding of variable coefficients', 3, None, '___sec79'),
              ('Verifying a constant solution',
               2,
               'decay:verify:trivial',
               'decay:verify:trivial'),
              ('Verification via manufactured solutions',
               2,
               'decay:MMS',
               'decay:MMS'),
              ('Computing convergence rates',
               2,
               'decay:convergence:rate',
               'decay:convergence:rate'),
              ('Estimating $r$', 3, None, '___sec83'),
              ('Implementation', 3, None, '___sec84'),
              ('Extension to systems of ODEs', 2, None, '___sec85'),
              ('General first-order ODEs', 1, None, '___sec86'),
              ('Generic form of first-order ODEs', 2, None, '___sec87'),
              ('The $\\theta$-rule', 2, None, '___sec88'),
              ('An implicit 2-step backward scheme', 2, None, '___sec89'),
              ('Leapfrog schemes', 2, None, '___sec90'),
              ('The ordinary Leapfrog scheme', 3, None, '___sec91'),
              ('The filtered Leapfrog scheme', 3, None, '___sec92'),
              ('The 2nd-order Runge-Kutta method', 2, None, '___sec93'),
              ('A 2nd-order Taylor-series method', 2, None, '___sec94'),
              ('The 2nd- and 3rd-order Adams-Bashforth schemes',
               2,
               None,
               '___sec95'),
              ('The 4th-order Runge-Kutta method',
               2,
               'decay:fd2:RK4',
               'decay:fd2:RK4'),
              ('The Odespy software', 2, None, '___sec97'),
              ('Example: Runge-Kutta methods', 2, None, '___sec98'),
              ('Remark about using the $\\theta$-rule in Odespy',
               3,
               None,
               '___sec99'),
              ('Example: Adaptive Runge-Kutta methods',
               2,
               'decay:fd2:adaptiveRK',
               'decay:fd2:adaptiveRK'),
              ('Exercises', 1, None, '___sec101'),
              ('Exercise 10: Experiment with precision in tests and the size of $u$',
               2,
               'decay:fd2:exer:precision',
               'decay:fd2:exer:precision'),
              ('Exercise 11: Implement the 2-step backward scheme',
               2,
               'decay:fd2:exer:bw2',
               'decay:fd2:exer:bw2'),
              ('Exercise 12: Implement the 2nd-order Adams-Bashforth scheme',
               2,
               'decay:fd2:exer:AB2',
               'decay:fd2:exer:AB2'),
              ('Exercise 13: Implement the 3rd-order Adams-Bashforth scheme',
               2,
               'decay:fd2:exer:AB3',
               'decay:fd2:exer:AB3'),
              ('Exercise 14: Analyze explicit 2nd-order methods',
               2,
               'decay:exer:RK2:Taylor:analysis',
               'decay:exer:RK2:Taylor:analysis'),
              ('Project 15: Implement and investigate the Leapfrog scheme',
               2,
               'decay:fd2:exer:leapfrog1',
               'decay:fd2:exer:leapfrog1'),
              ('Problem 16: Make a unified implementation of many schemes',
               2,
               'decay:fd2:exer:uni',
               'decay:fd2:exer:uni'),
              ('Models', 0, 'decay:app', 'decay:app'),
              ('Scaling', 1, 'decay:app:scaling', 'decay:app:scaling'),
              ('Dimensionless variables', 2, None, '___sec111'),
              ('Dimensionless numbers', 2, None, '___sec112'),
              ('A scaling for vanishing initial condition',
               2,
               None,
               '___sec113'),
              ('Evolution of a population',
               1,
               'decay:app:pop',
               'decay:app:pop'),
              ('Exponential growth',
               2,
               'decay:app:pop:exp',
               'decay:app:pop:exp'),
              ('Logistic growth',
               2,
               'decay:app:pop:log',
               'decay:app:pop:log'),
              ('Compound interest and inflation',
               1,
               'decay:app:interest',
               'decay:app:interest'),
              ("Newton's law of cooling",
               1,
               'decay:app:Newton:cooling',
               'decay:app:Newton:cooling'),
              ('Radioactive decay',
               1,
               'decay:app:nuclear',
               'decay:app:nuclear'),
              ('Deterministic model', 2, None, '___sec120'),
              ('Stochastic model', 2, None, '___sec121'),
              ('Relation between stochastic and deterministic models',
               2,
               None,
               '___sec122'),
              ('Generalization of the radioactive decay modeling',
               2,
               'decay:app:waitingtime',
               'decay:app:waitingtime'),
              ('Chemical kinetics',
               1,
               'decay:app:kinetics',
               'decay:app:kinetics'),
              ('Irreversible reaction of two substances',
               2,
               None,
               '___sec125'),
              ('Reversible reaction of two substances', 2, None, '___sec126'),
              ('Irreversible reaction of two substances into a third',
               2,
               None,
               '___sec127'),
              ('A biochemical reaction', 2, None, '___sec128'),
              ('Spreading of diseases', 1, 'decay:app:SIR', 'decay:app:SIR'),
              ('Predator-prey models in ecology',
               1,
               'decay:app:predprey',
               'decay:app:predprey'),
              ('Decay of atmospheric pressure with altitude',
               1,
               'decay:app:atm',
               'decay:app:atm'),
              ('The general model', 2, None, '___sec132'),
              ('Multiple atmospheric layers', 2, None, '___sec133'),
              ('Simplifications', 2, None, '___sec134'),
              ('Constant layer temperature', 3, None, '___sec135'),
              ('One-layer model', 3, None, '___sec136'),
              ('Compaction of sediments',
               1,
               'decay:app:sediment',
               'decay:app:sediment'),
              ('Vertical motion of a body in a viscous fluid',
               1,
               'decay:app:drag',
               'decay:app:drag'),
              ('Overview of forces', 2, None, '___sec139'),
              ('Equation of motion', 2, None, '___sec140'),
              ('Terminal velocity', 2, None, '___sec141'),
              ('A Crank-Nicolson scheme', 2, None, '___sec142'),
              ('Physical data', 2, None, '___sec143'),
              ('Verification', 2, None, '___sec144'),
              ('Scaling',
               2,
               'decay:app:drag:scaling',
               'decay:app:drag:scaling'),
              ('Viscoelastic materials',
               1,
               'decay:app:viscoelasticity',
               'decay:app:viscoelasticity'),
              ('Decay ODEs from solving a PDE by Fourier expansions',
               1,
               'decay:app:diffusion:Fourier',
               'decay:app:diffusion:Fourier'),
              ('Exercises', 1, None, '___sec148'),
              ('Problem 17: Radioactive decay of Carbon-14',
               2,
               'decay:app:exer:radio:C14',
               'decay:app:exer:radio:C14'),
              ("Exercise 18: Derive schemes for Newton's law of cooling",
               2,
               'decay:app:exer:cooling:schemes',
               'decay:app:exer:cooling:schemes'),
              ("Exercise 19: Implement schemes for Newton's law of cooling",
               2,
               'decay:app:exer:cooling:py',
               'decay:app:exer:cooling:py'),
              ('Exercise 20: Find time of murder from body temperature',
               2,
               'decay:app:exer:cooling:murder',
               'decay:app:exer:cooling:murder'),
              ('Exercise 21: Simulate an oscillating cooling process',
               2,
               'decay:app:exer:cooling:osc',
               'decay:app:exer:cooling:osc'),
              ('Exercise 22: Simulate stochastic radioactive decay',
               2,
               'decay:app:exer:stoch:nuclear',
               'decay:app:exer:stoch:nuclear'),
              ('Problem 23: Radioactive decay of two substances',
               2,
               'decay:app:exer:radio:twosubst',
               'decay:app:exer:radio:twosubst'),
              ('Exercise 24: Simulate a simple chemical reaction',
               2,
               'decay:app:exer:kinetics:AB',
               'decay:app:exer:kinetics:AB'),
              ('Exercise 25: Simulate an $n$-th order chemical reaction',
               2,
               'decay:app:exer:kinetics:ABn',
               'decay:app:exer:kinetics:ABn'),
              ('Exercise 26: Simulate a biochemical process',
               2,
               'decay:app:exer:MMK',
               'decay:app:exer:MMK'),
              ('Exercise 27: Simulate spreading of a disease',
               2,
               'decay:app:exer:SIR',
               'decay:app:exer:SIR'),
              ('Exercise 28: Simulate predator-prey interaction',
               2,
               'decay:app:exer:predprey',
               'decay:app:exer:predprey'),
              ('Exercise 29: Simulate the pressure drop in the atmosphere',
               2,
               'decay:app:exer:atm1',
               'decay:app:exer:atm1'),
              ('Exercise 30: Make a program for vertical motion in a fluid',
               2,
               'decay:app:exer:drag:prog',
               'decay:app:exer:drag:prog'),
              ('Project 31: Simulate parachuting',
               2,
               'decay:app:exer:parachute',
               'decay:app:exer:parachute'),
              ('Exercise 32: Formulate vertical motion in the atmosphere',
               2,
               'decay:app:exer:drag:atm1',
               'decay:app:exer:drag:atm1'),
              ('Exercise 33: Simulate vertical motion in the atmosphere',
               2,
               'decay:app:exer:drag:atm2',
               'decay:app:exer:drag:atm2'),
              ('Problem 34: Compute $y=|x|$ by solving an ODE',
               2,
               'decay:app:exer:signum',
               'decay:app:exer:signum'),
              ('Problem 35: Simulate fortune growth with random interest rate',
               2,
               'decay:app:exer:interest',
               'decay:app:exer:interest'),
              ('Exercise 36: Simulate a population in a changing environment',
               2,
               'decay:app:exer:pop:at',
               'decay:app:exer:pop:at'),
              ('Exercise 37: Simulate logistic growth',
               2,
               'decay:app:exer:pop:logistic1',
               'decay:app:exer:pop:logistic1'),
              ('Exercise 38: Rederive the equation for continuous compound interest',
               2,
               'decay:app:exer:interest:derive',
               'decay:app:exer:interest:derive'),
              ('Exercise 39: Simulate the deformation of a viscoelastic material',
               2,
               'decay:app:exer:viscoelasticity1',
               'decay:app:exer:viscoelasticity1'),
              ('Scientific software engineering', 0, 'decay:se', 'decay:se'),
              ('Implementations with functions and modules',
               1,
               'softeng1:basic',
               'softeng1:basic'),
              ('Mathematical problem and solution technique',
               2,
               'softeng1:basic:math',
               'softeng1:basic:math'),
              ('A first, quick implementation',
               2,
               'softeng1:basic:impl1',
               'softeng1:basic:impl1'),
              ('A more decent program',
               2,
               'softeng1:basic:impl2',
               'softeng1:basic:impl2'),
              ('Comments in a program', 3, None, '___sec177'),
              ('Refactoring into functions', 3, None, '___sec178'),
              ('Program file vs IDE vs notebook', 3, None, '___sec179'),
              ('Prefixing imported functions by the module name',
               2,
               'softeng1:basic:modprefix',
               'softeng1:basic:modprefix'),
              ('Implementing the numerical algorithm in a function',
               2,
               'softeng1:basic:func',
               'softeng1:basic:func'),
              ('Do not have several versions of a code',
               2,
               None,
               '___sec182'),
              ('Making a module',
               2,
               'softeng1:basic:module',
               'softeng1:basic:module'),
              ('Example on extending the module code',
               2,
               'softeng1:basic:experiment2',
               'softeng1:basic:experiment2'),
              ('Documenting functions and modules',
               2,
               'softeng1:basic:docstring',
               'softeng1:basic:docstring'),
              ('Logging intermediate results',
               2,
               'softeng1:basic:logging',
               'softeng1:basic:logging'),
              ('Introductory example', 3, None, '___sec187'),
              ('Using a logger in our solver function', 3, None, '___sec188'),
              ('User interfaces',
               1,
               'softeng1:basic:UI',
               'softeng1:basic:UI'),
              ('Command-line arguments', 2, None, '___sec190'),
              ('Positional command-line arguments', 2, None, '___sec191'),
              ('Option-value pairs on the command line',
               2,
               None,
               '___sec192'),
              ('Creating a graphical web user interface',
               2,
               None,
               '___sec193'),
              ('Making a compute function', 3, None, '___sec194'),
              ('Generating the user interface', 3, None, '___sec195'),
              ('Running the web application', 3, None, '___sec196'),
              ('Tests for verifying implementations', 1, None, '___sec197'),
              ('Doctests', 2, None, '___sec198'),
              ('Unit tests and test functions', 2, None, '___sec199'),
              ('Two Python test frameworks: nose and pytest',
               3,
               None,
               '___sec200'),
              ('Test function requirements', 3, None, '___sec201'),
              ('Comparison of real numbers', 3, None, '___sec202'),
              ('Special assert functions from nose', 3, None, '___sec203'),
              ('Locating test functions', 3, None, '___sec204'),
              ('Running tests', 3, None, '___sec205'),
              ('Embedding doctests in a test function', 3, None, '___sec206'),
              ('Installing nose and pytest', 3, None, '___sec207'),
              ('Test function for the solver', 2, None, '___sec208'),
              ('Test function for reading positional command-line arguments',
               2,
               None,
               '___sec209'),
              ('Test function for reading option-value pairs',
               2,
               None,
               '___sec210'),
              ('Classical class-based unit testing',
               2,
               'softeng1:basic:unittest',
               'softeng1:basic:unittest'),
              ('Sharing the software with other users',
               1,
               'softeng1:prog:se:git',
               'softeng1:prog:se:git'),
              ('Organizing the software directory tree',
               2,
               None,
               '___sec213'),
              ('Distributing just a module file', 3, None, '___sec214'),
              ('Distributing a package', 3, None, '___sec215'),
              ('Publishing the software at GitHub', 2, None, '___sec216'),
              ('Downloading and installing the software',
               2,
               None,
               '___sec217'),
              ('Installing just a module file', 3, None, '___sec218'),
              ('Installing a package', 3, None, '___sec219'),
              ('Classes for problem and solution method',
               1,
               'softeng1:prog:se:class',
               'softeng1:prog:se:class'),
              ('The problem class', 2, None, '___sec221'),
              ('The solver class', 2, None, '___sec222'),
              ('Combining the objects', 3, None, '___sec223'),
              ('Improving the problem and solver classes',
               2,
               'softeng1:prog:se:class2',
               'softeng1:prog:se:class2'),
              ('A generic class for parameters', 3, None, '___sec225'),
              ('Automating scientific experiments',
               1,
               'softeng1:experiments',
               'softeng1:experiments'),
              ('Available software', 2, None, '___sec227'),
              ('The results we want to produce', 2, None, '___sec228'),
              ('Combining plot files', 2, None, '___sec229'),
              ('Running a program from Python', 2, None, '___sec230'),
              ('The automating script', 2, None, '___sec231'),
              ('Making a report',
               2,
               'softeng1:exper:report',
               'softeng1:exper:report'),
              ('Word, OpenOffice, GoogleDocs', 3, None, '___sec233'),
              ('HTML with MathJax', 3, None, '___sec234'),
              ('LaTeX', 3, None, '___sec235'),
              ('Sphinx', 3, None, '___sec236'),
              ('Markdown', 3, None, '___sec237'),
              ('IPython/Jupyter notebooks', 3, None, '___sec238'),
              ('Wiki formats', 3, None, '___sec239'),
              ('DocOnce', 3, None, '___sec240'),
              ('Publishing a complete project',
               2,
               'softeng1:exper:github',
               'softeng1:exper:github'),
              ('Exercises', 1, None, '___sec242'),
              ('Problem 40: Make a tool for differentiating curves',
               2,
               'softeng1:exer:derivative',
               'softeng1:exer:derivative'),
              ('Problem 41: Make solid software for the Trapezoidal rule',
               2,
               'softeng1:exer:integral:flat',
               'softeng1:exer:integral:flat'),
              ('Problem 42: Implement classes for the Trapezoidal rule',
               2,
               'softeng1:exer:integral:flat2',
               'softeng1:exer:integral:flat2'),
              ('Problem 43: Write a doctest and a test function',
               2,
               'softeng1:exer:doctest1',
               'softeng1:exer:doctest1'),
              ('Problem 44: Investigate the size of tolerances in comparisons',
               2,
               'softeng1:exer:tol',
               'softeng1:exer:tol'),
              ('Remarks', 3, None, '___sec248'),
              ('Exercise 45: Make use of a class implementation',
               2,
               'softeng1:exer:class:dts',
               'softeng1:exer:class:dts'),
              ('Problem 46: Make solid software for a difference equation',
               2,
               'softeng1:exer:logistic',
               'softeng1:exer:logistic'),
              ('Summarizing multiple-choice questions', 0, None, '___sec251'),
              ('Quiz', 1, None, '___sec252'),
              ('Exercise 47: Characterize a finite difference',
               2,
               'decay:quiz:fd:FE',
               'decay:quiz:fd:FE'),
              ('Exercise 48: Characterize a finite difference',
               2,
               'decay:quiz:fd:CN',
               'decay:quiz:fd:CN'),
              ('Exercise 49: What is the problem with this program?',
               2,
               'decay:quiz:program',
               'decay:quiz:program'),
              ('Exercise 50: Is the solution correct?',
               2,
               'decay:quiz:stability',
               'decay:quiz:stability'),
              ('Exercise 51: Is this a proper test function?',
               2,
               'decay:quiz:testfunc',
               'decay:quiz:testfunc'),
              ('Exercise 52: Rewrite an expression with array arithmetics',
               2,
               'decay:quiz:arrayarithm',
               'decay:quiz:arrayarithm'),
              ('Exercise 53: What is the truncation error?',
               2,
               'decay:quiz:trunc',
               'decay:quiz:trunc'),
              ('Exercise 54: Recognize a programming language',
               2,
               'decay:quiz:prog:m',
               'decay:quiz:prog:m'),
              ('Exercise 55: Recognize a programming language',
               2,
               'decay:quiz:prog:py',
               'decay:quiz:prog:py'),
              ('Exercise 56: Recognize a programming language',
               2,
               'decay:quiz:prog:f77',
               'decay:quiz:prog:f77'),
              ('Exercise 57: Recognize a programming language',
               2,
               'decay:quiz:prog:c',
               'decay:quiz:prog:c'),
              ('Exercise 58: What is SymPy?',
               2,
               'decay:quiz:sympy1',
               'decay:quiz:sympy1'),
              ('Exercise 59: Testing of code',
               2,
               'decay:quiz:testing1',
               'decay:quiz:testing1'),
              ('Exercise 60: What kind of scheme is this?',
               2,
               'decay:quiz:scheme:CN_error',
               'decay:quiz:scheme:CN_error'),
              ('Exercise 61: What kind of scheme is this?',
               2,
               'decay:quiz:scheme:BE',
               'decay:quiz:scheme:BE'),
              ('Exercise 62: What kind of scheme is this?',
               2,
               'decay:quiz:scheme:leapfrog',
               'decay:quiz:scheme:leapfrog'),
              ('References', 1, None, '___sec269')]}
end of tocinfo -->

<body>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "none"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript"
 src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<!-- newcommands_keep.tex -->
$$
\newcommand{\half}{\frac{1}{2}}
\newcommand{\tp}{\thinspace .}
\newcommand{\uex}{{u_{\small\mbox{e}}}}
\newcommand{\Aex}{{A_{\small\mbox{e}}}}
\newcommand{\E}[1]{\hbox{E}\lbrack #1 \rbrack}
\newcommand{\Var}[1]{\hbox{Var}\lbrack #1 \rbrack}
\newcommand{\Std}[1]{\hbox{Std}\lbrack #1 \rbrack}
\newcommand{\Oof}[1]{\mathcal{O}(#1)}
\newcommand{\stress}{\boldsymbol{\sigma}}
$$




    
<a name="part0006"></a>
<p>
<!-- begin top navigation -->
<table style="width: 100%"><tr><td>
<div style="text-align: left;"><a href="._decay-book-solarized005.html">&laquo; Previous</a></div>
</td><td>
<div style="text-align: right;"><a href="._decay-book-solarized007.html">Next &raquo;</a></div>
</td></tr></table>
<!-- end top navigation -->
</p>

<p>
<!-- !split -->

<center><h1 id="decay:analysis">Analysis</h1></center> <hr>

<p>
We address the ODE for exponential decay,
$$
\begin{equation}
u'(t) = -au(t),\quad u(0)=I,
\tag{59}
\end{equation}
$$

where \( a \) and \( I \) are given constants. This problem is solved
by the \( \theta \)-rule finite difference scheme, resulting in
the recursive equations
$$
\begin{equation}
u^{n+1} = \frac{1 - (1-\theta) a\Delta t}{1 + \theta a\Delta t}u^n
\tag{60}
\end{equation}
$$

for the numerical solution \( u^{n+1} \), which approximates the exact
solution \( \uex \) at time point \( t_{n+1} \). For constant mesh spacing,
which we assume here, \( t_{n+1}=(n+1)\Delta t \).

<p>
The example programs associated with this chapter are found in
the directory <a href="http://tinyurl.com/ofkw6kc/analysis" target="_self"><tt>src/analysis</tt></a>.

<h1 id="___sec48">Experimental investigations </h1>

<p>
We first perform a series of numerical explorations to see how the
methods behave as we change the parameters \( I \), \( a \), and \( \Delta t \)
in the problem.

<h2 id="___sec49">Discouraging numerical solutions </h2>

<p>
Choosing \( I=1 \), \( a=2 \), and running experiments with \( \theta =1,0.5, 0 \)
for \( \Delta t=1.25, 0.75, 0.5, 0.1 \), gives the results in
Figures <a href="#decay:analysis:BE4c">10</a>, <a href="#decay:analysis:CN4c">11</a>, and
<a href="#decay:analysis:FE4c">12</a>.

<p>
<center> <!-- figure -->
<hr class="figure">
<center><p class="caption">Figure 10:  Backward Euler. <div id="decay:analysis:BE4c"></div> </p></center>
<p><img src="fig-analysis/BE4c.png" align="bottom" width=600></p>
</center>

<p>
<center> <!-- figure -->
<hr class="figure">
<center><p class="caption">Figure 11:  Crank-Nicolson. <div id="decay:analysis:CN4c"></div> </p></center>
<p><img src="fig-analysis/CN4c.png" align="bottom" width=600></p>
</center>

<p>
<center> <!-- figure -->
<hr class="figure">
<center><p class="caption">Figure 12:  Forward Euler. <div id="decay:analysis:FE4c"></div> </p></center>
<p><img src="fig-analysis/FE4c.png" align="bottom" width=600></p>
</center>

<p>
The characteristics of the displayed curves can be summarized as follows:

<ul>
  <li> The Backward Euler scheme gives a monotone solution in all cases,
    lying above the exact curve.</li>
  <li> The Crank-Nicolson scheme gives the most accurate results, but for
    \( \Delta t=1.25 \) the solution oscillates.</li>
  <li> The Forward Euler scheme gives a growing, oscillating solution for
    \( \Delta t=1.25 \); a decaying, oscillating solution for \( \Delta t=0.75 \);
    a strange solution \( u^n=0 \) for \( n\geq 1 \) when \( \Delta t=0.5 \); and
    a solution seemingly as accurate as the one by the Backward Euler
    scheme for \( \Delta t = 0.1 \), but the curve lies below the exact
    solution.</li>
</ul>

Since the exact solution of our model problem is a monotone function,
\( u(t)=Ie^{-at} \), some of these qualitatively wrong results indeed seem alarming!

<p>
<div class="alert alert-block alert-notice alert-text-normal">
<b>Key questions.</b>
<p>

<ul>
 <li> Under what circumstances, i.e., values of
   the input data \( I \), \( a \), and \( \Delta t \) will the Forward Euler and
   Crank-Nicolson schemes result in undesired oscillatory solutions?</li>
 <li> How does \( \Delta t \) impact the error in the numerical solution?</li>
</ul>

The first question will be investigated both by numerical experiments and
by precise mathematical theory. The theory will help establish
general criteria on \( \Delta t \) for avoiding non-physical oscillatory
or growing solutions.

<p>
For our simple model problem we can answer the second
question very precisely, but
we will also look at simplified formulas for small \( \Delta t \)
and touch upon important concepts such as <em>convergence rate</em> and
<em>the order of a scheme</em>. Other fundamental concepts mentioned are
stability, consistency, and convergence.
</div>


<h2 id="___sec50">Detailed experiments </h2>

<p>
To address the first question above,
we may set up an experiment where we loop over values of \( I \), \( a \),
and \( \Delta t \) in our chosen model problem.
For each experiment, we flag the solution as
oscillatory if

$$ u^{n} > u^{n-1},$$

for some value of \( n \). This seems like a reasonable choice,
since we expect \( u^n \) to decay with \( n \), but oscillations will make
\( u \) increase over a time step. Doing some initial experimentation
with varying \( I \), \( a \), and \( \Delta t \), quickly reveals that
oscillations are independent of \( I \), but they do depend on \( a \) and
\( \Delta t \). We can therefore limit the investigation to
vary \( a \) and \( \Delta t \). Based on this observation,
we introduce a two-dimensional
function \( B(a,\Delta t) \) which is 1 if oscillations occur
and 0 otherwise. We can visualize \( B \) as a contour plot
(lines for which \( B=\hbox{const} \)). The contour \( B=0.5 \)
corresponds to the borderline between oscillatory regions with \( B=1 \)
and monotone regions with \( B=0 \) in the \( a,\Delta t \) plane.

<p>
The \( B \) function is defined at discrete \( a \) and \( \Delta t \) values.
Say we have given \( P \) values for \( a \), \( a_0,\ldots,a_{P-1} \), and
\( Q \) values for \( \Delta t \), \( \Delta t_0,\ldots,\Delta t_{Q-1} \).
These \( a_i \) and \( \Delta t_j \) values, \( i=0,\ldots,P-1 \),
\( j=0,\ldots,Q-1 \), form a rectangular mesh of \( P\times Q \) points
in the plane spanned by \( a \) and \( \Delta t \).
At each point \( (a_i, \Delta t_j) \), we associate
the corresponding value \( B(a_i,\Delta t_j) \), denoted \( B_{ij} \).
The \( B_{ij} \) values are naturally stored in a two-dimensional
array. We can thereafter create a plot of the
contour line \( B_{ij}=0.5 \) dividing the oscillatory and monotone
regions. The file <a href="http://tinyurl.com/ofkw6kc/analysis/decay_osc_regions.py" target="_self"><tt>decay_osc_regions.py</tt></a>  given below (<code>osc_regions</code> stands for &quot;oscillatory regions&quot;) contains all nuts and
bolts to produce the \( B=0.5 \) line in Figures <a href="#decay:analysis:B:FE">13</a>
and <a href="#decay:analysis:B:CN">14</a>. The oscillatory region is above this line.

<p>
<!-- begin verbatim block  pypro-->
<pre><code>from decay_mod import solver
import numpy as np
import scitools.std as st

def non_physical_behavior(I, a, T, dt, theta):
    &quot;&quot;&quot;
    Given lists/arrays a and dt, and numbers I, dt, and theta,
    make a two-dimensional contour line B=0.5, where B=1&gt;0.5
    means oscillatory (unstable) solution, and B=0&lt;0.5 means
    monotone solution of u'=-au.
    &quot;&quot;&quot;
    a = np.asarray(a); dt = np.asarray(dt)  # must be arrays
    B = np.zeros((len(a), len(dt)))         # results
    for i in range(len(a)):
        for j in range(len(dt)):
            u, t = solver(I, a[i], T, dt[j], theta)
            # Does u have the right monotone decay properties?
            correct_qualitative_behavior = True
            for n in range(1, len(u)):
                if u[n] &gt; u[n-1]:  # Not decaying?
                    correct_qualitative_behavior = False
                    break  # Jump out of loop
            B[i,j] = float(correct_qualitative_behavior)
    a_, dt_ = st.ndgrid(a, dt)  # make mesh of a and dt values
    st.contour(a_, dt_, B, 1)
    st.grid('on')
    st.title('theta=%g' % theta)
    st.xlabel('a'); st.ylabel('dt')
    st.savefig('osc_region_theta_%s.png' % theta)
    st.savefig('osc_region_theta_%s.pdf' % theta)

non_physical_behavior(
    I=1,
    a=np.linspace(0.01, 4, 22),
    dt=np.linspace(0.01, 4, 22),
    T=6,
    theta=0.5)
</code></pre>
<!-- end verbatim block -->

<p>
<center> <!-- figure -->
<hr class="figure">
<center><p class="caption">Figure 13:  Forward Euler scheme: oscillatory solutions occur for points above the curve. <div id="decay:analysis:B:FE"></div> </p></center>
<p><img src="fig-analysis/osc_region_FE.png" align="bottom" width=500></p>
</center>

<p>
<center> <!-- figure -->
<hr class="figure">
<center><p class="caption">Figure 14:  Crank-Nicolson scheme: oscillatory solutions occur for points above the curve. <div id="decay:analysis:B:CN"></div> </p></center>
<p><img src="fig-analysis/osc_region_CN.png" align="bottom" width=500></p>
</center>

<p>
By looking at the curves in the figures one may guess that \( a\Delta t \)
must be less than a critical limit to avoid the undesired
oscillations.  This limit seems to be about 2 for Crank-Nicolson and 1
for Forward Euler.  We shall now establish a precise mathematical
analysis of the discrete model that can explain the observations in
our numerical experiments.

<h1 id="___sec51">Stability </h1>

<p>
The goal now is to understand the results in the previous section.
To this end, we shall investigate the properties of the mathematical
formula for the solution of the equations arising from the finite
difference methods.

<h2 id="___sec52">Exact numerical solution </h2>

<p>
Starting with \( u^0=I \), the simple recursion <a href="#mjx-eqn-60">(60)</a>
can be applied repeatedly \( n \) times, with the result that
$$
\begin{equation}
u^{n} = IA^n,\quad A = \frac{1 - (1-\theta) a\Delta t}{1 + \theta a\Delta t}\tp
\tag{61}
\end{equation}
$$

<p>
<div class="alert alert-block alert-notice alert-text-normal">
<b>Solving difference equations.</b>
<p>
Difference equations where all terms are linear in
\( u^{n+1} \), \( u^n \), and maybe \( u^{n-1} \), \( u^{n-2} \), etc., are
called <em>homogeneous, linear</em> difference equations, and their solutions
are generally of the form \( u^n=A^n \), where \( A \) is a constant to be
determined. Inserting this expression in the difference equation
and dividing by \( A^{n+1} \) gives
a polynomial equation in \( A \). In the present case we get

$$ A = \frac{1 - (1-\theta) a\Delta t}{1 + \theta a\Delta t}\tp $$

This is a solution technique of wider applicability than repeated use of
the recursion <a href="#mjx-eqn-60">(60)</a>.
</div>


<p>
Regardless of the solution approach, we have obtained a formula for
\( u^n \).  This formula can explain everything we see in the figures
above, but it also gives us a more general insight into accuracy and
stability properties of the three schemes.

<p>
Since \( u^n \) is a factor \( A \)
raised to an integer power \( n \), we realize that \( A < 0 \)
will imply \( u^n < 0 \) for odd \( n \) and \( u^n > 0 \) for even \( n \).
That is, the solution oscillates between the mesh points.
We have oscillations due to \( A < 0 \) when

$$
\begin{equation}
(1-\theta)a\Delta t > 1 \tp
\tag{62}
\end{equation}
$$

Since \( A>0 \) is a requirement for having a numerical solution with the
same basic property (monotonicity) as the exact solution, we may say
that \( A>0 \) is a <em>stability criterion</em>. Expressed in terms of \( \Delta t \)
the stability criterion reads

$$
\begin{equation}
\Delta t < \frac{1}{(1-\theta)a}\tp
\tag{63}
\end{equation}
$$

<p>
The Backward
Euler scheme is always stable since \( A < 0 \) is impossible for \( \theta=1 \), while
non-oscillating solutions for Forward Euler and Crank-Nicolson
demand \( \Delta t\leq 1/a \) and \( \Delta t\leq 2/a \), respectively.
The relation between \( \Delta t \) and \( a \) look reasonable: a larger
\( a \) means faster decay and hence a need for smaller time steps.

<p>
Looking at the upper left plot in Figure <a href="#decay:analysis:FE4c">12</a>,
we see that \( \Delta t=1.25 \), and remembering that \( a=2 \) in these
experiments, \( A \) can be calculated to be
\( -1.5 \), so the Forward Euler solution becomes \( u^n=(-1.5)^n \) (\( I=1 \)).
This solution oscillates <em>and</em> grows. The upper right plot has
\( a\Delta t = 2\cdot 0.75=1.5 \), so \( A=-0.5 \),
and \( u^n=(-0.5)^n \) decays but oscillates. The lower left plot
is a peculiar case where the Forward Euler scheme produces a solution
that is stuck on the \( t \) axis. Now we can understand why this is so,
because \( a\Delta t= 2\cdot 0.5=1 \), which gives \( A=0 \),
and therefore \( u^n=0 \) for \( n\geq 1 \).  The decaying oscillations in the Crank-Nicolson scheme in the upper left plot in Figure <a href="#decay:analysis:CN4c">11</a>
for \( \Delta t=1.25 \) are easily explained by the fact that \( A\approx -0.11 < 0 \).

<h2 id="___sec53">Stability properties derived from the amplification factor </h2>

<p>
The factor \( A \) is called the <em>amplification factor</em> since the solution
at a new time level is the solution at the previous time
level amplified by a factor \( A \).
For a decay process, we must obviously have \( |A|\leq 1 \), which
is fulfilled for all \( \Delta t \) if \( \theta \geq 1/2 \). Arbitrarily
large values of \( u \) can be generated when \( |A|>1 \) and \( n \) is large
enough. The numerical solution is in such cases totally irrelevant to
an ODE modeling decay processes! To avoid this situation, we must
demand \( |A|\leq 1 \) also for \( \theta < 1/2 \), which implies

$$
\begin{equation}
\Delta t \leq \frac{2}{(1-2\theta)a},
\tag{64}
\end{equation}
$$

For example, \( \Delta t \) must not exceed  \( 2/a \) when computing with
the Forward Euler scheme.

<p>
<div class="alert alert-block alert-notice alert-text-normal">
<b>Stability properties.</b>
<p>
We may summarize the stability investigations as follows:

<ol>
<li> The Forward Euler method is a <em>conditionally stable</em> scheme because
   it requires \( \Delta t < 2/a \) for avoiding growing solutions
   and \( \Delta t < 1/a \) for avoiding oscillatory solutions.</li>
<li> The Crank-Nicolson is <em>unconditionally stable</em> with respect to
   growing solutions, while it is conditionally stable with
   the criterion \( \Delta t < 2/a \) for avoiding oscillatory solutions.</li>
<li> The Backward Euler method is unconditionally stable with respect
   to growing and oscillatory solutions - any \( \Delta t \) will work.</li>
</ol>

Much literature on ODEs speaks about L-stable and A-stable methods.
In our case A-stable methods ensures non-growing solutions, while
L-stable methods also avoids oscillatory solutions.
</div>


<h1 id="___sec54">Accuracy </h1>

<p>
While stability concerns the qualitative properties of the numerical
solution, it remains to investigate the quantitative properties to
see exactly how large the numerical errors are.

<h2 id="___sec55">Visual comparison of amplification factors </h2>

<p>
After establishing how \( A \) impacts the qualitative features of the
solution, we shall now look more into how well the numerical amplification
factor approximates the exact one. The exact solution reads
\( u(t)=Ie^{-at} \), which can be rewritten as
$$
\begin{equation}
{\uex}(t_n) = Ie^{-a n\Delta t} = I(e^{-a\Delta t})^n \tp
\tag{65}
\end{equation}
$$

From this formula we see that the exact amplification factor is
$$
\begin{equation}
\Aex = e^{-a\Delta t} \tp
\tag{66}
\end{equation}
$$

<p>
We see from all of our analysis
that the exact and numerical amplification factors depend
on \( a \) and \( \Delta t \) through the dimensionless
product \( a\Delta t \): whenever there is a
\( \Delta t \) in the analysis, there is always an associated \( a \)
parameter. Therefore, it
is convenient to introduce a symbol for this product, \( p=a\Delta t \),
and view \( A \) and \( \Aex \) as functions of \( p \). Figure
<a href="#decay:analysis:fig:A">15</a> shows these functions. The two amplification
factors are clearly closest for the
Crank-Nicolson method, but that method has
the unfortunate oscillatory behavior when \( p>2 \).

<p>
<center> <!-- figure -->
<hr class="figure">
<center><p class="caption">Figure 15:  Comparison of amplification factors. <div id="decay:analysis:fig:A"></div> </p></center>
<p><img src="fig-analysis/A_factors.png" align="bottom" width=500></p>
</center>

<p>
<div class="alert alert-block alert-notice alert-text-normal">
<b>Significance of the \( p=a\Delta t \) parameter.</b>
<p>
The key parameter for numerical performance of a scheme is in this model
problem \( p=a\Delta t \). This is a <em>dimensionless number</em> (\( a \) has dimension
1/s and \( \Delta t \) has dimension s) reflecting how the discretization
parameter plays together with a physical parameter in the problem.

<p>
One can bring the present model problem on dimensionless form
through a process called scaling. The scaled modeled has a modified
time \( \bar t = at \) and modified response \( \bar u =u/I \) such that
the model reads \( d\bar u/d\bar t = -\bar u \), \( \bar u(0)=1 \).
Analyzing this model, where there are no physical parameters,
we find that \( \Delta \bar t \) is the key parameter
for numerical performance. In the unscaled model,
this corresponds to \( \Delta \bar t = a\Delta t \).

<p>
It is common that the numerical performance of methods for solving ordinary and
partial differential equations is governed by dimensionless parameters
that combine mesh sizes with physical parameters.
</div>


<h2 id="___sec56">Series expansion of amplification factors </h2>

<p>
As an alternative to the visual understanding inherent in Figure
<a href="#decay:analysis:fig:A">15</a>, there is a strong tradition in numerical
analysis to establish formulas for approximation errors when the
discretization parameter, here \( \Delta t \), becomes small. In the
present case, we let \( p \) be our small discretization parameter, and it
makes sense to simplify the expressions for \( A \) and \( \Aex \) by using
Taylor polynomials around \( p=0 \).  The Taylor polynomials are accurate
for small \( p \) and greatly simplify the comparison of the analytical
expressions since we then can compare polynomials, term by term.

<p>
Calculating the Taylor series for \( \Aex \) is easily done by hand, but
the three versions of \( A \) for \( \theta=0,1,{\half} \) lead to more
cumbersome calculations.
Nowadays, analytical computations can benefit greatly by
symbolic computer algebra software. The Python package <code>sympy</code>
represents a powerful computer algebra system, not yet as sophisticated as
the famous Maple and Mathematica systems, but it is free and
very easy to integrate with our numerical computations in Python.

<p>
When using <code>sympy</code>, it is convenient to enter an interactive Python
shell where the results of expressions and statements can be shown
immediately.
Here is a simple example. We strongly recommend to use
<code>isympy</code> (or <code>ipython</code>) for such interactive sessions.

<p>
Let us illustrate <code>sympy</code> with a standard Python shell syntax
(<code>&gt;&gt;&gt;</code> prompt) to compute a Taylor polynomial approximation to \( e^{-p} \):

<p>
<!-- begin verbatim block  pyshell-->
<pre><code>&gt;&gt;&gt; from sympy import *
&gt;&gt;&gt; # Create p as a mathematical symbol with name 'p'
&gt;&gt;&gt; p = Symbols('p')
&gt;&gt;&gt; # Create a mathematical expression with p
&gt;&gt;&gt; A_e = exp(-p)
&gt;&gt;&gt;
&gt;&gt;&gt; # Find the first 6 terms of the Taylor series of A_e
&gt;&gt;&gt; A_e.series(p, 0, 6)
1 + (1/2)*p**2 - p - 1/6*p**3 - 1/120*p**5 + (1/24)*p**4 + O(p**6)
</code></pre>
<!-- end verbatim block -->
Lines with <code>&gt;&gt;&gt;</code> represent input lines, whereas without
this prompt represent the result of the previous command (note that
<code>isympy</code> and <code>ipython</code> apply other prompts, but in this text
we always apply <code>&gt;&gt;&gt;</code> for interactive Python computing).
Apart from the order of the powers, the computed formula is easily
recognized as the beginning of the Taylor series for \( e^{-p} \).

<p>
Let us define the numerical amplification factor where \( p \) and \( \theta \)
enter the formula as symbols:

<p>
<!-- begin verbatim block  pyshell-->
<pre><code>&gt;&gt;&gt; theta = Symbol('theta')
&gt;&gt;&gt; A = (1-(1-theta)*p)/(1+theta*p)
</code></pre>
<!-- end verbatim block -->
To work with the factor for the Backward Euler scheme we
can substitute the value 1 for <code>theta</code>:

<p>
<!-- begin verbatim block  pyshell-->
<pre><code>&gt;&gt;&gt; A.subs(theta, 1)
1/(1 + p)
</code></pre>
<!-- end verbatim block -->
Similarly, we can substitute <code>theta</code> by 1/2 for Crank-Nicolson,
preferably using an exact rational representation of 1/2 in <code>sympy</code>:

<p>
<!-- begin verbatim block  pyshell-->
<pre><code>&gt;&gt;&gt; half = Rational(1,2)
&gt;&gt;&gt; A.subs(theta, half)
1/(1 + (1/2)*p)*(1 - 1/2*p)
</code></pre>
<!-- end verbatim block -->

<p>
The Taylor series of the amplification factor for the Crank-Nicolson
scheme can be computed as

<p>
<!-- begin verbatim block  pyshell-->
<pre><code>&gt;&gt;&gt; A.subs(theta, half).series(p, 0, 4)
1 + (1/2)*p**2 - p - 1/4*p**3 + O(p**4)
</code></pre>
<!-- end verbatim block -->
We are now in a position to compare Taylor series:

<p>
<!-- begin verbatim block  pyshell-->
<pre><code>&gt;&gt;&gt; FE = A_e.series(p, 0, 4) - A.subs(theta, 0).series(p, 0, 4)
&gt;&gt;&gt; BE = A_e.series(p, 0, 4) - A.subs(theta, 1).series(p, 0, 4)
&gt;&gt;&gt; CN = A_e.series(p, 0, 4) - A.subs(theta, half).series(p, 0, 4 )
&gt;&gt;&gt; FE
(1/2)*p**2 - 1/6*p**3 + O(p**4)
&gt;&gt;&gt; BE
-1/2*p**2 + (5/6)*p**3 + O(p**4)
&gt;&gt;&gt; CN
(1/12)*p**3 + O(p**4)
</code></pre>
<!-- end verbatim block -->
From these expressions we see that the error \( A-\Aex\sim \Oof{p^2} \)
for the Forward and Backward Euler schemes, while
\( A-\Aex\sim \Oof{p^3} \) for the Crank-Nicolson scheme.
The notation \( \Oof{p^m} \) here means a polynomial in \( p \) where
\( p^m \) is the term of lowest-degree, and consequently the term that
dominates the expression for \( p < 0 \). We call this the
<em>leading order term</em>. As \( p\rightarrow 0 \), the leading order term
clearly dominates over the higher-order terms (think of \( p=0.01 \):
\( p \) is a hundred times larger than \( p^2 \)).

<p>
Now, \( a \) is a given parameter in the problem, while \( \Delta t \) is
what we can vary. Not surprisingly, the error expressions are usually
written in terms \( \Delta t \). We then have

$$
\begin{equation}
A-\Aex = \left\lbrace\begin{array}{ll}
\Oof{\Delta t^2}, & \hbox{Forward and Backward Euler},\\ 
\Oof{\Delta t^3}, & \hbox{Crank-Nicolson}
\end{array}\right.
\tag{67}
\end{equation}
$$

<p>
We say that the Crank-Nicolson scheme has an error in the amplification
factor of order \( \Delta t^3 \), while the two other schemes are
of order \( \Delta t^2 \) in the same quantity.

<p>
What is the significance of the order expression? If we halve \( \Delta t \),
the error in amplification factor at a time level will be reduced
by a factor of 4 in the Forward and Backward Euler schemes, and by
a factor of 8 in the Crank-Nicolson scheme. That is, as we
reduce \( \Delta t \) to obtain more accurate results, the Crank-Nicolson
scheme reduces the error more efficiently than the other schemes.

<h2 id="___sec57">The ratio of numerical and exact amplification factors </h2>

<p>
An alternative comparison of the schemes is provided by looking at the
ratio \( A/\Aex \), or the error \( 1-A/\Aex \) in this ratio:

<p>
<!-- begin verbatim block  pyshell-->
<pre><code>&gt;&gt;&gt; FE = 1 - (A.subs(theta, 0)/A_e).series(p, 0, 4)
&gt;&gt;&gt; BE = 1 - (A.subs(theta, 1)/A_e).series(p, 0, 4)
&gt;&gt;&gt; CN = 1 - (A.subs(theta, half)/A_e).series(p, 0, 4)
&gt;&gt;&gt; FE
(1/2)*p**2 + (1/3)*p**3 + O(p**4)
&gt;&gt;&gt; BE
-1/2*p**2 + (1/3)*p**3 + O(p**4)
&gt;&gt;&gt; CN
(1/12)*p**3 + O(p**4)
</code></pre>
<!-- end verbatim block -->
The leading-order terms have the same powers as
in the analysis of \( A-\Aex \).

<h2 id="decay:analysis:gobal:error">The global error at a point</h2>

<p>
The error in the amplification factor reflects the error when
progressing from time level \( t_n \) to \( t_{n-1} \) only. That is,
we disregard the error already present in the solution at \( t_{n-1} \).
The real error at a point, however, depends on the error development
over all previous time steps. This error,
\( e^n = u^n-\uex(t_n) \), is known as the <em>global error</em>. We
may look at \( u^n \) for some \( n \) and Taylor expand the
mathematical expressions as functions of \( p=a\Delta t \) to get a simple
expression for the global error (for small \( p \)). Continuing the
<code>sympy</code> expression from previous section, we can write

<p>
<!-- begin verbatim block  pyshell-->
<pre><code>&gt;&gt;&gt; n = Symbol('n')
&gt;&gt;&gt; u_e = exp(-p*n)
&gt;&gt;&gt; u_n = A**n
&gt;&gt;&gt; FE = u_e.series(p, 0, 4) - u_n.subs(theta, 0).series(p, 0, 4)
&gt;&gt;&gt; BE = u_e.series(p, 0, 4) - u_n.subs(theta, 1).series(p, 0, 4)
&gt;&gt;&gt; CN = u_e.series(p, 0, 4) - u_n.subs(theta, half).series(p, 0, 4)
&gt;&gt;&gt; FE
(1/2)*n*p**2 - 1/2*n**2*p**3 + (1/3)*n*p**3 + O(p**4)
&gt;&gt;&gt; BE
(1/2)*n**2*p**3 - 1/2*n*p**2 + (1/3)*n*p**3 + O(p**4)
&gt;&gt;&gt; CN
(1/12)*n*p**3 + O(p**4)
</code></pre>
<!-- end verbatim block -->
Note that <code>sympy</code> does not sort the polynomial terms in the output,
so \( p^3 \) appears before \( p^2 \) in the output of <code>BE</code>.

<p>
For a fixed time \( t \), the parameter \( n \) in these expressions increases
as \( p\rightarrow 0 \) since \( t=n\Delta t =\mbox{const} \) and hence
\( n \) must increase like \( \Delta t^{-1} \). With \( n \) substituted by
\( t/\Delta t \) in
the leading-order error terms, these become

$$
\begin{align}
e^n &= \half n p^2 = {\half}ta^2\Delta t, &\hbox{Forward Euler}
\tag{68}\\ 
e^n &= -\half n p^2 = -{\half}ta^2\Delta t, &\hbox{Backward Euler}
\tag{69}\\ 
e^n &= \frac{1}{12}np^3 = \frac{1}{12}ta^3\Delta t^2, &\hbox{Crank-Nicolson}
\tag{70}
\end{align}
$$

The global error is therefore of
second order (in \( \Delta t \)) for the Crank-Nicolson scheme and of
first order for the other two schemes.

<p>
<div class="alert alert-block alert-notice alert-text-normal">
<b>Convergence.</b>
<p>
When the global error \( e^n\rightarrow 0 \) as \( \Delta t\rightarrow 0 \),
we say that the scheme is <em>convergent</em>. It means that the numerical
solution approaches the exact solution as the mesh is refined, and
this is a much desired property of a numerical method.
</div>


<h2 id="decay:analysis:gobal:error_int">Integrated error</h2>

<p>
It is common to study the norm of the numerical error, as
explained in detail in the section <a href="._decay-book-solarized004.html#decay:computing:error:norm">Computing the norm of the error mesh function</a>.
The \( L^2 \) norm of the error can be computed by treating \( e^n \) as a function
of \( t \) in <code>sympy</code> and performing symbolic integration.
From now on we shall do <code>import sympy as sym</code> and prefix all functions
in <code>sympy</code> by <code>sym</code> to explicitly notify ourselves that the functions
are from <code>sympy</code>. This is particularly advantageous when we use
mathematical functions like <code>sin</code>: <code>sym.sin</code> is for symbolic expressions,
while <code>sin</code> from <code>numpy</code> or <code>math</code> is for numerical computation.
For the Forward Euler scheme we have

<p>
<!-- begin verbatim block  pycod-->
<pre><code>import sympy as sym
p, n, a, dt, t, T, theta = sym.symbols('p n a dt t T theta')
A = (1-(1-theta)*p)/(1+theta*p)
u_e = sym.exp(-p*n)
u_n = A**n
error = u_e.series(p, 0, 4) - u_n.subs(theta, 0).series(p, 0, 4)
# Introduce t and dt instead of n and p
error = error.subs('n', 't/dt').subs(p, 'a*dt')
error = error.as_leading_term(dt) # study only the first term
print error
error_L2 = sym.sqrt(sym.integrate(error**2, (t, 0, T)))
print 'L2 error:', sym.simplify(error_error_L2)
</code></pre>
<!-- end verbatim block -->
The output reads

<p>
<!-- begin verbatim block -->
<pre><code>sqrt(30)*sqrt(T**3*a**4*dt**2*(6*T**2*a**2 - 15*T*a + 10))/60
</code></pre>
<!-- end verbatim block -->
which means that the \( L^2 \) error behaves like \( a^2\Delta t \).

<p>
Strictly speaking, the numerical error is only defined at the
mesh points so it makes most sense to compute the
\( \ell^2 \) error

$$ ||e^n||_{\ell^2} = \sqrt{\Delta t\sum_{n=0}^{N_t} ({\uex}(t_n) - u^n)^2}
\tp $$

We have obtained an exact analytical expression for the error at
\( t=t_n \), but here we use the leading-order error term only since we
are mostly interested in how the error behaves as a polynomial in
\( \Delta t \) or \( p \), and then the leading order term will dominate.  For
the Forward Euler scheme, \( \uex(t_n) - u^n \approx {\half}np^2 \), and
we have

$$
||e^n||_{\ell^2}^2 = \Delta t\sum_{n=0}^{N_t} \frac{1}{4}n^2p^4
=\Delta t\frac{1}{4}p^4 \sum_{n=0}^{N_t} n^2\tp
$$

Now, \( \sum_{n=0}^{N_t} n^2\approx \frac{1}{3}N_t^3 \). Using this approximation,
setting \( N_t =T/\Delta t \), and taking the square root gives the expression

$$
\begin{equation}
||e^n||_{\ell^2} = \half\sqrt{\frac{T^3}{3}} a^2\Delta t\tp
\tag{71}
\end{equation}
$$

Calculations for the Backward Euler scheme are very similar and provide
the same result, while the Crank-Nicolson scheme leads to

$$
\begin{equation}
||e^n||_{\ell^2} = \frac{1}{12}\sqrt{\frac{T^3}{3}}a^3\Delta t^2\tp
\tag{72}
\end{equation}
$$

<p>
<div class="alert alert-block alert-summary alert-text-normal">
<b>Summary of errors.</b>
<p>
Both the global point-wise errors <a href="#mjx-eqn-68">(68)</a>-<a href="#mjx-eqn-70">(70)</a>
and their time-integrated versions <a href="#mjx-eqn-71">(71)</a> and <a href="#mjx-eqn-72">(72)</a> show that

<ul>
 <li> the Crank-Nicolson scheme is of second order in \( \Delta t \), and</li>
 <li> the Forward Euler and Backward Euler schemes are of first order in \( \Delta t \).</li>
</ul>
</div>


<h2 id="decay:analysis:trunc">Truncation error</h2>

<p>
The truncation error is a very frequently used error measure for
finite difference methods. It is defined as <em>the error
in the difference equation that arises when inserting the exact
solution</em>. Contrary to many other error measures, e.g., the
true error \( e^n=\uex(t_n)-u^n \), the truncation error is a quantity that
is easily computable.

<p>
Before reading on, it is wise to review the section <a href="._decay-book-solarized003.html#decay:fd:taylor">Mathematical derivation of finite difference formulas</a>
on how Taylor polynomials were used to derive finite differences and
quantify the error in the formulas. Very similar reasoning, and almost
identical mathematical details, will be carried out below, but in a slightly
different context. Now, the
focus is on the error when solving a differential
equation, while in the section <a href="._decay-book-solarized003.html#decay:fd:taylor">Mathematical derivation of finite difference formulas</a> we derived
errors for a finite difference formula. These errors are tightly
connected in the present model problem.

<p>
Let us illustrate the calculation of the truncation error
for the Forward Euler scheme.
We start with the difference equation on operator form,

$$ \lbrack D_t^+ u = -au\rbrack^n,$$

which is the short form for

$$ \frac{u^{n+1}-u^n}{\Delta t} = -au^n\tp$$

The idea is to see how well the exact solution \( \uex(t) \) fulfills
this equation. Since \( \uex(t) \) in general will not obey the
discrete equation, we get an error in the discrete equation. This
error is called
a <em>residual</em>, denoted here by \( R^n \):

$$
\begin{equation}
R^n = \frac{\uex(t_{n+1})-\uex(t_n)}{\Delta t} + a\uex(t_n)
\tp
\tag{73}
\end{equation}
$$

The residual is defined at each mesh point and is therefore a mesh
function with a superscript \( n \).

<p>
The interesting feature of \( R^n \) is to see how it
depends on the discretization parameter \( \Delta t \).
The tool for reaching
this goal is to Taylor expand \( \uex \) around the point where the
difference equation is supposed to hold, here \( t=t_n \).
We have that

$$ \uex(t_{n+1}) = \uex(t_n) + \uex'(t_n)\Delta t + \half\uex''(t_n)
\Delta t^2 + \cdots, $$

which may be used to reformulate the fraction in
<a href="#mjx-eqn-73">(73)</a> so that

$$ R^n = \uex'(t_n) + \half\uex''(t_n)\Delta t + \ldots + a\uex(t_n)\tp$$

Now, \( \uex \) fulfills the ODE \( \uex'=-a\uex \), which means that the first and last
term cancel and we have

$$ R^n = \half\uex''(t_n)\Delta t + \Oof{\Delta t^2}\tp $$

This \( R^n \) is the <em>truncation error</em>, which for the Forward Euler is seen
to be of first order in \( \Delta t \) as \( \Delta \rightarrow 0 \).

<p>
The above procedure can be repeated for the Backward Euler and the
Crank-Nicolson schemes. We start with the scheme in operator notation,
write it out in detail, Taylor expand \( \uex \) around the point \( \tilde t \)
at which the difference equation is defined, collect terms that
correspond to the ODE (here \( \uex' + a\uex \)), and identify the remaining
terms as the residual \( R \), which is the truncation error.
The Backward Euler scheme leads to

$$ R^n \approx -\half\uex''(t_n)\Delta t, $$

while the Crank-Nicolson scheme gives

$$ R^{n+\half} \approx \frac{1}{24}\uex'''(t_{n+\half})\Delta t^2,$$

when \( \Delta t\rightarrow 0 \).

<p>
The <em>order</em> \( r \) of a finite difference scheme is often defined through
the leading term \( \Delta t^r \) in the truncation error. The above
expressions point out that the Forward and Backward Euler schemes are
of first order, while Crank-Nicolson is of second order.  We have
looked at other error measures in other sections, like the error in
amplification factor and the error \( e^n=\uex(t_n)-u^n \), and expressed
these error measures in terms of \( \Delta t \) to see the order of the
method. Normally, calculating the truncation error is more
straightforward than deriving the expressions for other error measures
and therefore the easiest way to establish the order of a scheme.

<h2 id="___sec61">Consistency, stability, and convergence </h2>

<p>
Three fundamental concepts when solving differential equations by
numerical methods are consistency, stability, and convergence.  We
shall briefly touch upon these concepts below in the context of the present
model problem.

<p>
Consistency means that the error in the difference equation, measured
through the truncation error, goes to zero as \( \Delta t\rightarrow
0 \). Since the truncation error tells how well the exact solution
fulfills the difference equation, and the exact solution fulfills the
differential equation, consistency ensures that the difference
equation approaches the differential equation in the limit. The
expressions for the truncation errors in the previous section are all
proportional to \( \Delta t \) or \( \Delta t^2 \), hence they vanish as
\( \Delta t\rightarrow 0 \), and all the schemes are consistent.  Lack of
consistency implies that we actually solve some other differential
equation in the limit \( \Delta t\rightarrow 0 \) than we aim at.

<p>
Stability means that the numerical solution exhibits the same
qualitative properties as the exact solution. This is obviously a
feature we want the numerical solution to have. In the present
exponential decay model, the exact solution is monotone and
decaying. An increasing numerical solution is not in accordance with
the decaying nature of the exact solution and hence unstable. We can
also say that an oscillating numerical solution lacks the property of
monotonicity of the exact solution and is also unstable. We have seen
that the Backward Euler scheme always leads to monotone and decaying
solutions, regardless of \( \Delta t \), and is hence stable. The Forward
Euler scheme can lead to increasing solutions and oscillating
solutions if \( \Delta t \) is too large and is therefore unstable unless
\( \Delta t \) is sufficiently small.  The Crank-Nicolson can never lead
to increasing solutions and has no problem to fulfill that stability
property, but it can produce oscillating solutions and is unstable in
that sense, unless \( \Delta t \) is sufficiently small.

<p>
Convergence implies that the global (true) error mesh function \( e^n =
\uex(t_n)-u^n\rightarrow 0 \) as \( \Delta t\rightarrow 0 \). This is really
what we want: the numerical solution gets as close to the exact
solution as we request by having a sufficiently fine mesh.

<p>
Convergence is hard to establish theoretically, except in quite simple
problems like the present one. Stability and consistency are much
easier to calculate. A major breakthrough in the understanding of
numerical methods for differential equations came in 1956 when Lax and
Richtmeyer established equivalence between convergence on one hand and
consistency and stability on the other (the <a href="http://en.wikipedia.org/wiki/Lax_equivalence_theorem" target="_self">Lax equivalence theorem</a>).  In practice
it meant that one can first establish that a method is stable and
consistent, and then it is automatically convergent (which is much
harder to establish).  The result holds for linear problems only, and
in the world of nonlinear differential equations the relations between
consistency, stability, and convergence are much more complicated.

<p>
We have seen in the previous analysis that the Forward Euler,
Backward Euler, and Crank-Nicolson schemes are convergent (\( e^n\rightarrow 0 \)),
that they are consistent (\( R^n\rightarrow 0 \)), and that they are
stable under certain conditions on the size of \( \Delta t \).
We have also derived explicit mathematical expressions for \( e^n \),
the truncation error, and the stability criteria.

<p>
<!-- Look in Asher and Petzold, p 40 -->

<h1 id="___sec62">Various types of errors in a differential equation model </h1>

<p>
So far we have been concerned with one type of error, namely the
discretization error committed by replacing the differential equation
problem by a recursive set of difference equations. There are,
however, other types of errors that must be considered too. We can
classify errors into four groups:

<ol>
<li> model errors: how wrong is the ODE model?</li>
<li> data errors: how wrong are the input parameters?</li>
<li> discretization errors: how wrong is the numerical method?</li>
<li> rounding errors: how wrong is the computer arithmetics?</li>
</ol>

Below, we shall briefly describe and illustrate these four types
of errors. Each of the errors deserve its own chapter, at least,
so the treatment here is superficial to give some indication
about the nature of size of the errors in a specific case.
Some of the required computer codes quickly become more advanced
than in the rest of the book, but we include to code to document
all the details that lie behind the investigations of the errors.

<h2 id="___sec63">Model errors </h2>

<p>
Any mathematical model like \( u^{\prime}=-au \), \( u(0)=I \), is just an
approximate description of a real-world phenomenon. How good this
approximation is can be determined by comparing physical experiments
with what the model predicts. This is the topic of <em>validation</em> and is
obviously an essential part of mathematical model. One difficulty with
validation is that we need to estimate the parameters in the model, and
this brings in data errors. Quantifying data errors is challenging,
and a frequently used method is to <em>tune</em> the parameters in the model
to make model predictions as close as possible to the experiments.
That is, we do not attempt to measure or estimate all input
parameters, but instead find values that &quot;make the model good&quot;.
Another difficulty is that the response in experiments also contains
errors due to measurement techniques.

<p>
Let us try to quantify model errors in a very simple example involving
\( u^{\prime}=-au \), \( u(0)=I \), with constant \( a \).  Suppose a more
accurate model has \( a \) as a function of time rather than a
constant. Here we take \( a(t) \) as a simple linear function: \( a +
pt \). Obviously, \( u \) with \( p>0 \) will go faster to zero with time than a
constant \( a \).

<p>
The solution of

$$ u^{\prime} = (a + pt)u,\quad u(0)=I,$$

can be shown (see below) to be

$$ u(t) = I e^{-t \left(a + \half pt\right)}\tp$$

Let a Python function <code>true_model(t, I, a, p)</code> implement the
above \( u(t) \) and let
the solution of our primary ODE \( u^{\prime}=-au \) be available as
the function <code>model(t, I, a)</code>.
We can now make some plots of the two models and the error for some values
of \( p \). Figure <a href="#decay:analysis:model_errors:fig:model_u">16</a> displays
<code>model</code> versus <code>true_model</code> for \( p=0.01, 0.1, 1 \), while Figure
<a href="#decay:analysis:model_errors:fig:model_e">17</a> shows the difference
between the two models as a function of \( t \) for the same \( p \) values.

<p>
<center> <!-- figure -->
<hr class="figure">
<center><p class="caption">Figure 16:  Comparison of two models for three values of \( p \). <div id="decay:analysis:model_errors:fig:model_u"></div> </p></center>
<p><img src="fig-analysis/model_errors_u.png" align="bottom" width=800></p>
</center>

<p>
<center> <!-- figure -->
<hr class="figure">
<center><p class="caption">Figure 17:  Discrepancy of Comparison of two models for three values of \( p \). <div id="decay:analysis:model_errors:fig:model_e"></div> </p></center>
<p><img src="fig-analysis/model_errors_e.png" align="bottom" width=500></p>
</center>

<p>
The code that was used to produce the plots looks like

<p>
<!-- begin verbatim block  pycod-->
<pre><code>from numpy import linspace, exp
from matplotlib.pyplot import \ 
     plot, show, xlabel, ylabel, legend, savefig, figure, title

def model_errors():
    p_values = [0.01, 0.1, 1]
    a = 1
    I = 1
    t = linspace(0, 4, 101)
    legends = []
    # Work with figure(1) for the discrepancy and figure(2+i)
    # for plotting the model and the true model for p value no i
    for i, p in enumerate(p_values):
        u = model(t, I, a)
        u_true = true_model(t, I, a, p)
        discrepancy = u_true - u
        figure(1)
        plot(t, discrepancy)
        figure(2+i)
        plot(t, u, 'r-', t, u_true, 'b--')
        legends.append('p=%g' % p)
    figure(1)
    legend(legends, loc='lower right')
    savefig('tmp1.png'); savefig('tmp1.pdf')
    for i, p in enumerate(p_values):
        figure(2+i)
        legend(['model', 'true model'])
        title('p=%g' % p)
        savefig('tmp%d.png' % (2+i)); savefig('tmp%d.pdf' % (2+i))
</code></pre>
<!-- end verbatim block -->

<p>
To derive the analytical solution of the model \( u^{\prime}=-(a+pt)u \), \( 
u(0)=I \), we can use SymPy and the code below. This is somewhat advanced
SymPy use for a newbie, but serves to illustrate the possibilities to
solve differential equations by symbolic software.

<p>
<!-- begin verbatim block  pycod-->
<pre><code>def derive_true_solution():
    import sympy as sym
    u = sym.symbols('u', cls=sym.Function)  # function u(t)
    t, a, p, I = sym.symbols('t a p I', real=True)

    def ode(u, t, a, p):
        &quot;&quot;&quot;Define ODE: u' = (a + p*t)*u. Return residual.&quot;&quot;&quot;
        return sym.diff(u, t) + (a + p*t)*u

    eq = ode(u(t), t, a, p)
    s = sym.dsolve(eq)
    # s is sym.Eq object u(t) == expression, we want u = expression,
    # so grab the right-hand side of the equality (Eq obj.)
    u = s.rhs
    print u
    # u contains C1, replace it with a symbol we can fit to
    # the initial condition
    C1 = sym.symbols('C1', real=True)
    u = u.subs('C1', C1)
    print u
    # Initial condition equation
    eq = u.subs(t, 0) - I
    s = sym.solve(eq, C1)  # solve eq wrt C1
    print s
    # s is a list s[0] = ...
    # Replace C1 in u by the solution
    u = u.subs(C1, s[0])
    print 'u:', u
    print sym.latex(u)  # latex formula for reports

    # Consistency check: u must fulfill ODE and initial condition
    print 'ODE is fulfilled:', sym.simplify(ode(u, t, a, p))
    print 'u(0)-I:', sym.simplify(u.subs(t, 0) - I)

    # Convert u expression to Python numerical function
    # (modules='numpy' allows numpy arrays as arguments,
    # we want this for t)
    u_func = sym.lambdify([t, I, a, p], u, modules='numpy')
    return u_func

true_model = derive_true_solution()
</code></pre>
<!-- end verbatim block -->

<h2 id="___sec64">Data errors </h2>

<p>
By &quot;data&quot; we mean all the input parameters to a model, in our case
\( I \) and \( a \). The values of these may contain errors, or at least
uncertainty. Suppose \( I \) and \( a \) are measured from some physical
experiments. Ideally, we have many samples of \( I \) and \( a \) and
from these we can fit probability distributions. Assume that \( I \)
turns out to be normally distributed with mean 1 and standard deviation 0.2,
while \( a \) is uniformly distributed in the interval \( [0.5, 1.5] \).

<p>
How will the uncertainty in \( I \) and \( a \) propagate through the model
\( u=Ie^{-at} \)? That is, what is the uncertainty in \( u \) at a particular
time \( t \)? This answer can easily be answered using <em>Monte Carlo
simulation</em>. It means that we draw a lot of samples from the
distributions for \( I \) and \( a \). For each combination of \( I \) and \( a \)
sample we compute the corresponding \( u \) value for selected values of
\( t \).  Afterwards, we can for each selected \( t \) values make a histogram
of all the computed \( u \) values to see what the distribution of \( u \)
values look like. Figure <a href="#decay:analysis:data_errors:fig">18</a> shows the
histograms corresponding to \( t=0,1,3 \). We see that the distribution of
\( u \) values is much like a symmetric normal distribution at \( t=0 \),
centered around \( u=1 \). At later times, the distribution gets more
asymmetric and narrower. It means that the uncertainty decreases with
time.

<p>
From the computed \( u \) values we can easily calculate the mean and
standard deviation. The table below shows the mean and standard
deviation values along with the value if we just use the formula
\( u=Ie^{-at} \) with the mean values of \( I \) and \( a \): \( I=1 \) and \( a=1 \). As
we see, there is some discrepancy between this latter (naive)
computation and the mean value produced by Monte Carlo simulation.

<p>
<table border="1">
<thead>
<tr><th align="center">time</th> <th align="center">mean</th> <th align="center">st.dev.</th> <td align="center">\( u(t;I=a=1) \)</td> </tr>
</thead>
<tbody>
<tr><td align="left">   0       </td> <td align="left">   1.00    </td> <td align="left">   0.200      </td> <td align="left">   1.00                </td> </tr>
<tr><td align="left">   1       </td> <td align="left">   0.38    </td> <td align="left">   0.135      </td> <td align="left">   0.37                </td> </tr>
<tr><td align="left">   3       </td> <td align="left">   0.07    </td> <td align="left">   0.060      </td> <td align="left">   0.14                </td> </tr>
</tbody>
</table>
<p>
Actually, \( u(t;I,a) \) becomes a stochastic variable for each \( t \) when
\( I \) and \( a \) are stochastic variables, as they are in the above
Monte Carlo simulation. The mean of the stochastic \( u(t;I,a) \) is
not equal to \( u \) with mean values of the input data, \( u(t;I=a=1) \),
unless \( u \) is linear in \( I \) and \( a \) (here \( u \) is nonlinear in \( a \)).

<p>
<center> <!-- figure -->
<hr class="figure">
<center><p class="caption">Figure 18:  Histogram of solution uncertainty at three time points, due to data errors. <div id="decay:analysis:data_errors:fig"></div> </p></center>
<p><img src="fig-analysis/data_errors.png" align="bottom" width=800></p>
</center>

<p>
Estimating statistical uncertainty in input data and investigating
how this uncertainty
propagates to uncertainty in the response of a differential equation model
(or other models) are key topics in the scientific field called
<em>uncertainty quantification</em>, simply known as UQ.
Estimation of the statistical properties of input data can either be
done directly from physical experiments, or one can find the
parameter values that provide a &quot;best fit&quot; of model predictions with
experiments. Monte Carlo simulation is a general and widely used
tool to solve the associated statistical problems.
The accuracy of the Monte Carlo results increases with increasing
number of samples \( N \), typically the error behaves like \( N^{-1/2} \).

<p>
The computer code required to do the Monte Carlo simulation and
produce the plots in Figure <a href="#decay:analysis:data_errors:fig">18</a>
is shown below.

<p>
<!-- begin verbatim block  pycod-->
<pre><code>def data_errors():
    from numpy import random, mean, std
    from matplotlib.pyplot import hist
    N = 10000
    # Draw random numbers for I and a
    I_values = random.normal(1, 0.2, N)
    a_values = random.uniform(0.5, 1.5, N)
    # Compute corresponding u values for some t values
    t = [0, 1, 3]
    u_values = {}  # samples for various t values
    u_mean = {}
    u_std = {}
    for t_ in t:
        # Compute u samples corresponding to I and a samples
        u_values[t_] = [model(t_, I, a)
                        for I, a in zip(I_values, a_values)]
        u_mean[t_] = mean(u_values[t_])
        u_std[t_] = std(u_values[t_])

        figure()
        dummy1, bins, dummy2 = hist(
            u_values[t_], bins=30, range=(0, I_values.max()),
            normed=True, facecolor='green')
        #plot(bins)
        title('t=%g' % t_)
        savefig('tmp_%g.png' % t_); savefig('tmp_%g.pdf' % t_)
    # Table of mean and standard deviation values
    print 'time   mean   st.dev.'
    for t_ in t:
        print '%3g    %.2f    %.3f' % (t_, u_mean[t_], u_std[t_])
</code></pre>
<!-- end verbatim block -->

<h2 id="___sec65">Discretization errors </h2>

<p>
The errors implied by solving the differential equation problem by
the \( \theta \)-rule has been thoroughly analyzed in the previous
sections. Below are some plots of the error versus time for the
Forward Euler (FE), Backward Euler (BN), and Crank-Nicolson (CN)
schemes for decreasing values of \( \Delta t \). Since the difference
in magnitude between the errors in the CN scheme versus the FE and
BN schemes grows significantly as \( \Delta t \) is reduced (the error
goes like \( \Delta t^2 \) for CN versus \( \Delta t \) for FE/BE), we have
plotted the logarithm of the absolute value of the numerical error
as a mesh function.

<p>
<center> <!-- figure -->
<hr class="figure">
<center><p class="caption">Figure 19:  Discretization errors in various schemes for four time step values. <div id="decay:analysis:numerical_errors:fig"></div> </p></center>
<p><img src="fig-analysis/numerical_errors.png" align="bottom" width=700></p>
</center>

<p>
The table below presents exact figures of the discretization error
for various choices of \( \Delta t \) and schemes.

<p>
<table border="1">
<thead>
<tr><td align="center">\( \Delta t \)</td> <th align="center">         FE         </th> <th align="center">         BE         </th> <th align="center">        CN       </th> </tr>
</thead>
<tbody>
<tr><td align="right">   0.4               </td> <td align="right">   \( 9\cdot 10^{-2} \)    </td> <td align="right">   \( 6\cdot 10^{-2} \)    </td> <td align="right">   $ 5\cdot 10^{-3}$    </td> </tr>
<tr><td align="right">   0.1               </td> <td align="right">   \( 2\cdot 10^{-2} \)    </td> <td align="right">   \( 2\cdot 10^{-2} \)    </td> <td align="right">   $ 3\cdot 10^{-4}$    </td> </tr>
<tr><td align="right">   0.01              </td> <td align="right">   \( 2\cdot 10^{-3} \)    </td> <td align="right">   \( 2\cdot 10^{-3} \)    </td> <td align="right">   $ 3\cdot 10^{-6}$    </td> </tr>
</tbody>
</table>
<p>
The computer code used to generate the plots appear next. It makes use
of a <code>solver</code> function
as shown in the section <a href="._decay-book-solarized004.html#decay:py2">Integer division</a>.

<p>
<!-- begin verbatim block  pycod-->
<pre><code>def discretization_errors():
    from numpy import log, abs
    I = 1
    a = 1
    T = 4
    t = linspace(0, T, 101)
    schemes = {'FE': 0, 'BE': 1, 'CN': 0.5}  # theta to scheme name
    dt_values = [0.8, 0.4, 0.1, 0.01]
    for dt in dt_values:
        figure()
        legends = []
        for scheme in schemes:
            theta = schemes[scheme]
            u, t = solver(I, a, T, dt, theta)
            u_e = model(t, I, a)
            error = u_e - u
            print '%s: dt=%.2f, %d steps, max error: %.2E' % \ 
                  (scheme, dt, len(u)-1, abs(error).max())
            # Plot log(error), but exclude error[0] since it is 0
            plot(t[1:], log(abs(error[1:])))
            legends.append(scheme)
        xlabel('t');  ylabel('log(abs(numerical error))')
        legend(legends, loc='upper right')
        title(r'$\Delta t=%g$' % dt)
        savefig('tmp_dt%g.png' % dt); savefig('tmp_dt%g.pdf' % dt)
</code></pre>
<!-- end verbatim block -->

<h2 id="___sec66">Rounding errors </h2>

<p>
Real numbers on a computer are represented by <a href="https://en.wikipedia.org/wiki/Floating_point" target="_self">floating-point
numbers</a>, which means
that just a finite number of digits are stored and used. Therefore,
the floating-point number is an approximation to the underlying real
number. When doing arithmetics with floating-point numbers, there will
be small approximation errors, called round-off errors or rounding
errors, that may or may not accumulate in comprehensive computations.

<p>
The cause and analysis of rounding errors are described in most
books on numerical analysis, see for instance
Chapter 2 in Gander et al. <a href="._decay-book-solarized011.html#Gander_2015">[5]</a>. For very simple
algorithms it is possible to theoretically establish bounds for
the rounding errors, but for most algorithms one cannot know to
what extent rounding errors accumulate and potentially destroy
the final answer. <a href="#decay:analysis:exer:rounding">Problem 9: Explore rounding errors in numerical calculus</a>
demonstrates the impact of rounding errors on numerical
differentiation and integration.

<p>
Here is a simplest possible example of the effect of rounding
errors:

<p>
<!-- begin verbatim block  pyshell-->
<pre><code>&gt;&gt;&gt; 1.0/51*51
1.0
&gt;&gt;&gt; 1.0/49*49
0.9999999999999999
</code></pre>
<!-- end verbatim block -->
We see that the latter result is not exact, but features an
error of \( 10^{-16} \). This is the typical level of a rounding error
from an arithmetic operation with the widely used
64 bit floating-point number
(<code>float</code> object in Python, often called <code>double</code> or double precision
in other languages). One cannot expect more accuracy than \( 10^{-16} \).
The big question is if errors at this level accumulate in a given
numerical algorithm.

<p>
What is the effect of using <code>float</code> objects and not exact arithmetics
when solving differential equations? We can investigate this question
through computer experiments if we have the ability to represent real
numbers to a desired accuracy. Fortunately, Python has a <code>Decimal</code>
object in the <a href="https://docs.python.org/2/library/decimal.html" target="_self"><tt>decimal</tt></a> module that allows us
to use as many digits in floating-point numbers as we like. We take
1000 digits as the true answer where rounding errors are negligible,
and then we run our numerical algorithm (the Crank-Nicolson scheme to
be precise) with <code>Decimal</code> objects for all real numbers and compute
the maximum error arising from using 4, 16, 64, and 128 digits.

<p>
When computing with numbers around unity in size and doing \( N_t=40 \) time
steps, we typically get a rounding error of \( 10^{-d} \), where \( d \) is
the number of digits used. The effect of rounding errors may
accumulate if we perform more operations, so increasing the number of
time steps to 4000 gives a rounding error of the order \( 10^{-d+2} \).
Also, if we compute with numbers that are much larger than unity, we
lose accuracy due to rounding errors. For example, for the \( u \) values
implied by \( I=1000 \) and \( a=100 \) (\( u\sim 10^3 \)),
the rounding errors increase to about
\( 10^{-d+3} \). Below is a table summarizing a set of experiments. A
rough model for the size of rounding errors is \( 10^{-d+q+r} \), where
\( d \) is the number of digits, the number of time steps is of the order
\( 10^q \) time steps, and the size of the numbers in the arithmetic
expressions are of order \( 10^r \).

<p>
<table border="1">
<thead>
<tr><th align="center">digits</th> <td align="center">\( u\sim 1 \), \( N_t=40 \)</td> <td align="center">\( u\sim 1 \), \( N_t=4000 \)</td> <td align="center">\( u\sim 10^3 \), \( N_t=40 \)</td> <td align="center">\( u\sim 10^3 \), \( N_t=4000 \)</td> </tr>
</thead>
<tbody>
<tr><td align="right">   4         </td> <td align="left">   \( 3.05\cdot 10^{-4} \)        </td> <td align="left">   \( 2.51\cdot 10^{-1} \)          </td> <td align="left">   \( 3.05\cdot 10^{-1} \)           </td> <td align="left">   \( 9.82\cdot 10^{2} \)              </td> </tr>
<tr><td align="right">   16        </td> <td align="left">   \( 1.71\cdot 10^{-16} \)       </td> <td align="left">   \( 1.42\cdot 10^{-14} \)         </td> <td align="left">   \( 1.58\cdot 10^{-13} \)          </td> <td align="left">   \( 4.84\cdot 10^{-11} \)            </td> </tr>
<tr><td align="right">   64        </td> <td align="left">   \( 2.99\cdot 10^{-64} \)       </td> <td align="left">   \( 1.80\cdot 10^{-62} \)         </td> <td align="left">   \( 2.06\cdot 10^{-61} \)          </td> <td align="left">   \( 1.04\cdot 10^{-57} \)            </td> </tr>
<tr><td align="right">   128       </td> <td align="left">   \( 1.60\cdot 10^{-128} \)      </td> <td align="left">   \( 1.56\cdot 10^{-126} \)        </td> <td align="left">   \( 2.41\cdot 10^{-125} \)         </td> <td align="left">   \( 1.07\cdot 10^{-122} \)           </td> </tr>
</tbody>
</table>
<p>
We realize that rounding errors are at the lowest possible level
if we scale the differential equation model,
see the section <a href="._decay-book-solarized008.html#decay:app:scaling">Scaling</a>,
so the numbers entering the computations are of unity in size,
and if we take a small number of steps (40 steps gives a discretization error
of \( 5\cdot 10^{-3} \) with the Crank-Nicolson scheme).
In general, rounding errors are negligible in comparison with other errors
in differential equation models.

<p>
The computer code for doing the reported experiments need a new version
of the <code>solver</code> function where we do arithmetics with <code>Decimal</code>
objects:

<p>
<!-- begin verbatim block  pycod-->
<pre><code>def solver_decimal(I, a, T, dt, theta):
    &quot;&quot;&quot;Solve u'=-a*u, u(0)=I, for t in (0,T] with steps of dt.&quot;&quot;&quot;
    from numpy import zeros, linspace
    from decimal import Decimal as D
    dt = D(dt)
    a = D(a)
    theta = D(theta)
    Nt = int(round(D(T)/dt))
    T = Nt*dt
    u = zeros(Nt+1, dtype=object)  # array of Decimal objects
    t = linspace(0, float(T), Nt+1)

    u[0] = D(I)               # assign initial condition
    for n in range(0, Nt):    # n=0,1,...,Nt-1
        u[n+1] = (1 - (1-theta)*a*dt)/(1 + theta*dt*a)*u[n]
    return u, t
</code></pre>
<!-- end verbatim block -->
The function below carries out the experiments. We can conveniently
set the number of digits as we want through the <code>decimal.getcontext().prec</code>
variable.

<p>
<!-- begin verbatim block  pycod-->
<pre><code>def rounding_errors(I=1, a=1, T=4, dt=0.1):
    import decimal
    from numpy import log, array, abs
    digits_values = [4, 16, 64, 128]
    # &quot;Exact&quot; arithmetics is taken as 1000 decimals here
    decimal.getcontext().prec = 1000
    u_e, t = solver_decimal(I=I, a=a, T=T, dt=dt, theta=0.5)
    for digits in digits_values:
        decimal.getcontext().prec = digits  # set no of digits
        u, t = solver_decimal(I=I, a=a, T=T, dt=dt, theta=0.5)
        error = u_e - u
        error = array(error[1:], dtype=float)
        print '%d digits, %d steps, max abs(error): %.2E' % \ 
              (digits, len(u)-1, abs(error).max())
</code></pre>
<!-- end verbatim block -->

<h2 id="___sec67">Discussion of the size of various errors </h2>

<p>
The previous computational examples of model, data, discretization,
and rounding errors are tied to one particular mathematical problem,
so it is in principle dangerous to make general conclusions.  However,
the illustrations made point to some common trends that apply to
differential equation models.

<p>
First, rounding errors have very little impact compared to the other
types of errors.  Second, numerical errors are in general smaller than
model and data errors, but more importantly, numerical errors are
often well understood and can be reduced by just increasing the
computational work (in our example by taking more smaller time steps).

<p>
Third, data errors may be significant, and it also takes a significant
amount of computational work to quantify them and their impact on the
solution. Many types of input data are also difficult or impossible
to measure, so finding suitable values requires tuning of the data
and the model to a known (measured) response.
Nevertheless, even if the predictive precision of a model
is limited because of severe errors or uncertainty in input data, the
model can still be of high value for investigating qualitative
properties of the underlying phenomenon. Through computer experiments
with synthetic input data one can understand a lot of the science or
engineering that goes into the model.

<p>
Fourth, model errors are the most challenging type of error to deal
with. Simplicity of model is in general preferred over complexity,
but adding complexity is often the only way to improve the predictive
capabilities of a model. More complexity usually also means a need
for more input data and consequently the danger of increasing
data errors.

<h1 id="___sec68">Exercises </h1>

<p>
<!-- --- begin exercise --- -->

<h2 id="decay:analysis:exer:fd:exp:plot">Problem 7: Visualize the accuracy of finite differences</h2>

<p>
The purpose of this exercise is to visualize the accuracy of finite difference
approximations of the derivative of a given function.
For any finite difference approximation, take the Forward Euler difference
as an example, and any specific function, take  \( u=e^{-at} \),
we may introduce an error fraction

$$
\begin{align*}
E = \frac{[D_t^+ u]^n}{u'(t_n)} &= \frac{\exp{(-a(t_n+\Delta t))} - \exp{(-at_n)}}{-a\exp{(-at_n)\Delta t}}\\ 
&= \frac{1}{a\Delta t}\left(1 -\exp{(-a\Delta t)}\right),
\end{align*}
$$

and view \( E \) as a function of \( \Delta t \). We expect that
\( \lim_{\Delta t\rightarrow 0}E=1 \), while \( E \) may deviate significantly from
unity for large \( \Delta t \). How the error depends on \( \Delta t \) is best
visualized in a graph where we use a logarithmic scale for \( \Delta t \),
so we can cover many orders of magnitude of that quantity. Here is
a code segment creating an array of 100 intervals, on the logarithmic
scale, ranging from \( 10^{-6} \) to \( 10^{-0.5} \) and then plotting \( E \) versus
\( p=a\Delta t \) with logarithmic scale on the \( p \) axis:

<p>
<!-- begin verbatim block  pycod-->
<pre><code>from numpy import logspace, exp
from matplotlib.pyplot import semilogx
p = logspace(-6, -0.5, 101)
y = (1-exp(-p))/p
semilogx(p, y)
</code></pre>
<!-- end verbatim block -->
Illustrate such errors for the finite difference operators \( [D_t^+u]^n \)
(forward), \( [D_t^-u]^n \) (backward), and \( [D_t u]^n \) (centered) in
the same plot.

<p>
Perform a Taylor series expansions of the error fractions and find
the leading order \( r \) in the expressions of type
\( 1 + Cp^r + \Oof{p^{r+1}} \), where \( C \) is some constant.

<p>
<!-- --- begin hint in exercise --- -->

<p>
<b>Hint.</b>
To save manual calculations and learn more about symbolic computing,
make functions for the three difference operators and use <code>sympy</code>
to perform the symbolic differences, differentiation, and Taylor series
expansion. To plot a symbolic expression <code>E</code> against <code>p</code>, convert the
expression to a Python function first: <code>E = sympy.lamdify([p], E)</code>.

<p>
<!-- --- end hint in exercise --- -->

<p>
<!-- removed !bsol ... !esol environment (because of the command-line option --without_solutions)
 -->Filename: <code>decay_plot_fd_error</code>.

<p>
<!-- --- end exercise --- -->

<p>
<!-- --- begin exercise --- -->

<h2 id="decay:analysis:exer:growth">Problem 8: Explore the \( \theta \)-rule for exponential growth</h2>

<p>
This exercise asks you to solve the ODE \( u'=-au \) with \( a < 0 \) such that
the ODE models exponential growth instead of exponential decay.  A
central theme is to investigate numerical artifacts and non-physical
solution behavior.

<p>
<b>a)</b>
Set \( a=-1 \) and run experiments with \( \theta=0, 0.5, 1 \) for
various values of \( \Delta t \) to uncover numerical artifacts.
Recall that the exact solution is a
monotone, growing function when \( a < 0 \). Oscillations or significantly
wrong growth are signs of wrong qualitative behavior.

<p>
From the experiments, select four values of \( \Delta t \) that
demonstrate the kind of numerical solutions that are characteristic
for this model.

<p>
<!-- removed !bsol ... !esol environment (because of the command-line option --without_solutions)
 -->
<p>
<b>b)</b>
Write up the amplification factor and plot it for \( \theta=0,0.5,1 \)
together with the exact one for \( a\Delta t < 0 \). Use the plot to
explain the observations made in the experiments.

<p>
<!-- --- begin hint in exercise --- -->

<p>
<b>Hint.</b>
Modify the <a href="http://tinyurl.com/ofkw6kc/analysis/decay_ampf_plot.py" target="_self"><tt>decay_ampf_plot.py</tt></a> code
(in the <code>src/analysis</code> directory).

<p>
<!-- --- end hint in exercise --- -->

<p>
<!-- removed !bsol ... !esol environment (because of the command-line option --without_solutions)
 -->
<p>
Filename: <code>exponential_growth</code>.

<p>
<!-- --- end exercise --- -->

<p>
<!-- --- begin exercise --- -->

<h2 id="decay:analysis:exer:rounding">Problem 9: Explore rounding errors in numerical calculus</h2>

<p>
<b>a)</b>
Compute the absolute values of the errors in the numerical derivative
of \( e^{-t} \) at \( t=\half \) for three types of finite difference
approximations: a forward difference, a backward difference, and
a centered difference, for \( \Delta t = 2^{-k} \), \( k=0,4, 8, 12, \ldots, 60 \).
When do rounding errors destroy the accuracy?

<p>
<!-- removed !bsol ... !esol environment (because of the command-line option --without_solutions)
 -->
<p>
<b>b)</b>
Compute the absolute values of the errors in the numerical
approximation of \( \int_0^4 e^{-t}dt \) using the Trapezoidal
and the Midpoint integration methods. Make a table of
the errors for \( n=2^k \) intervals, \( k=1,3,5=ldots,21 \).
Is there any impact of rounding errors?

<p>
<!-- --- begin hint in exercise --- -->

<p>
<b>Hint.</b>
The Trapezoidal rule for \( \int_a^bf(x)dx \) reads

$$ \int_a^bf(x)dx\approx h(\half f(a) + \half f(b) + \sum_{i=1}^{n-1}
f(a+ih)),\quad h = \frac{b-a}{n}\tp$$

The Midpoint rule is

$$ \int_a^bf(x)dx\approx h\sum_{i=1}^n f(a + (i+\half)h)\tp$$

<p>
<!-- --- end hint in exercise --- -->

<p>
<!-- removed !bsol ... !esol environment (because of the command-line option --without_solutions)
 -->
<p>
Filename: <code>rounding</code>.

<p>
<!-- --- end exercise --- -->

<p>
<p>
<!-- begin bottom navigation -->
<table style="width: 100%"><tr><td>
<div style="text-align: left;"><a href="._decay-book-solarized005.html">&laquo; Previous</a></div>
</td><td>
<div style="text-align: right;"><a href="._decay-book-solarized007.html">Next &raquo;</a></div>
</td></tr></table>
<!-- end bottom navigation -->
</p>

<!-- ------------------- end of main content --------------- -->


<center style="font-size:80%">
<!-- copyright only on the titlepage -->
</center>


</body>
</html>
    
