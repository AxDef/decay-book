======= Various types of errors in a differential equation model =======

So far we have been concerned with one type of error, namely the
discretization error committed by replacing the differential equation
problem by a recursive set of difference equations. There are,
however, other types of errors that must be considered too. We can
classify errors into four groups:

 o model errors
 o data errors
 o discretization errors
 o round-off errors

Below, we shall briefly describe and illustrate these four types
of errors.

===== Model errors =====

Any mathematical model like $u^{\prime}=-au$, $u(0)=I$, is just an
approximate description of a real-world phenomenon. Suppose a more
accurate model has $a$ as a function of time rather than a
constant. Here we take $a(t)$ as a simple linear function: $a +
pt$. Obviously, $u$ with $p>0$ will go faster to zero with time than a
constant $a$.

The solution of

!bt
\[ u^{\prime} = (a + pt)u,\quad u(0)=I,\]
!et
can be shown (see below) to be

!bt
\[ u(t) = I e^{t \left(- a - \frac{p t}{2}\right)}\tp\]
!et
With the above $u$ available in a Python function `u_true(t, I, a, p)`
and the solution from our model $u^{\prime}=-au$ available in `u(t, I, a)`,
we can make some plots of the two models and the error for some values
of $p$. Figure ref{decay:analysis:model_errors:fig:model_u} displays
the two curves for $p=0.01, 0.1, 1$, while Figure
ref{decay:analysis:model_errors:fig:model_e} shows the difference
between the two models as a function of $t$ for the same $p$ values.

FIGURE: [fig-analysis/model_errors_u, width=800 frac=1] Comparison of two models for three values of $p$. label{decay:analysis:model_errors:fig:model_u}

FIGURE: [fig-analysis/model_errors_e, width=500 frac=0.8] Discrepancy of Comparison of two models for three values of $p$. label{decay:analysis:model_errors:fig:model_e}

The code that was used to produce the plots looks like

@@@CODE src-analysis/errors.py fromto: from numpy import@def data_errors

To derive the analytical solution of the model $u^{\prime}=-(a+pt)u$, $
u(0)=I$, we can use SymPy and the code below. This is somewhat advanced
SymPy use for a newbie, but serves to illustrate the possibilities to
solve differential equations by symbolic software.

@@@CODE src-analysis/errors.py fromto: def derive_true_solution@from numpy import linspace

===== Data errors =====

idx{Monte Carlo simulation}

By ``data'' we mean all the input parameters to a model, in our case
$I$ and $a$. The values of these may contain errors, or at least
uncertainty. Suppose $I$ and $a$ are measured from some physical
experiments. Ideally, we have many samples of $I$ and $a$ and
from these we can fit probability distributions. Assume that $I$
turns out to be normally distributed with mean 1 and standard deviation 0.2,
while $a$ is uniformly distributed in the interval $[0.5, 1.5]$.

How will the uncertainty in $I$ and $a$ propagate through the model
$u=Ie^{-at}$? That is, what is the uncertainty in $u$ at a particular
time $t$? This answer can easily be answered using *Monte Carlo
simulation*. It means that we draw a lot of samples from the
distributions for $I$ and $a$. For each combination of $I$ and $a$
sample we compute the corresponding $u$ value for selected values of
$t$.  Afterwards, we can for each selected $t$ values make a histogram
of all the computed $u$ values to see what the distribution of $u$
values look like. Figure ref{decay:analysis:data_errors:fig} shows the
histograms corresponding to $t=0,1,3$. We see that the distribution of
$u$ values is much like a symmetric normal distribution at $t=0$,
centered around $u=1$. At later times, the distribution gets more
asymmetric and narrower. It means that the uncertainty decreases with
time. From the computed $u$ values we can easily calculate the mean
and standard deviation. The table below shows the mean and standard
deviation values along with the value if we just use the formula
$u=Ie^{-at}$ with the mean values of $I$ and $a$: $I=1$ and $a=1$. As
we see, there is some discrepancy between this latter (naive)
computation and the mean value produced by Monte Carlo simulation.

|------------------------------------------|
| time | mean |  st.dev. | $u(t;I=1,a=1)$  |
|--l------l--------l---------l-------------|
|  0   | 1.00 |  0.200   | 1.00            |
|  1   | 0.38 |  0.135   | 0.37            |
|  3   | 0.07 |  0.060   | 0.14            |
|------------------------------------------|

Actually, $u(t;I,a)$ becomes a stochastic variable for each $t$ when
$I$ and $a$ are stochastic variables, as they are in the above
Monte Carlo simulation. The mean of the stochastic $u(t;I,a)$ is
not equal to $u(t;I=1,a=1)$ unless $u$ is linear in $I$ and $a$
(here $u$ is nonlinear in $a$). The accuracy of the Monte Carlo
results increases with increasing number of samples.

FIGURE: [fig-analysis/data_errors, width=800 frac=1] Histogram of solution uncertainty at three time points, due to data errors. label{decay:analysis:data_errors:fig}

The computer code required to do the Monte Carlo simulation and
produce the plots in Figure ref{decay:analysis:data_errors:fig}
is shown below.

@@@CODE src-analysis/errors.py fromto: def data_errors@def solver

===== Discretization errors =====

The errors implied by solving the differential equation problem by
the $\theta$-rule has been thoroughly analyzed in the previous
sections. Below are some plots of the error versus time for the
Forward Euler (FE), Backward Euler (BN), and Crank-Nicolson (CN)
schemes for decreasing values of $\Delta t$. Since the difference
in magnitude between the errors in the CN scheme versus the FE ad
BN schemes grows significantly as $\Delta t$ is reduced (the error
goes like $\Delta t^2$ for CN versus $\Delta t$ for FE/BE), we have
plotted the logarithm of the absolute value of the numerical error.

FIGURE: [fig-analysis/numerical_errors, width=700 frac=0.9] Discretization errors in various schemes for 4 time step values. label{decay:analysis:numerical_errors:fig}

The table below presents exact figures of the discretization error
for various choices of $\Delta t$ and schemes.

|----------------------------------------------------------------------|
| $\Delta t$ |        FE        |        BE        |        CN         |
|----r---------------r------------------r------------------r-----------|
| 0.4        | $9\cdot 10^{-2}$ | $6\cdot 10^{-2}$ | $ 5\cdot 10^{-3}$ |
| 0.1        | $2\cdot 10^{-2}$ | $2\cdot 10^{-2}$ | $ 3\cdot 10^{-4}$ |
| 0.01       | $2\cdot 10^{-3}$ | $2\cdot 10^{-3}$ | $ 3\cdot 10^{-6}$ |
|----------------------------------------------------------------------|

The computer code used to generate the plots appear next. It makes use
of a `solver` function
% if DOCUMENT == "document":
for computing the numerical solution of $u^{\prime}=-au$ with
the $\theta$-rule.
% else:
as shown in Section ref{decay:py2}.
% endif

@@@CODE src-analysis/errors.py fromto: def discretization_errors@def solver_decimal

===== Rounding errors =====

Real numbers on a computer are represented by "floating-point numbers": "https://en.wikipedia.org/wiki/Floating_point", which means that just a finite
number of digits are stored and used. Therefore, the floating-point number
is an approximation to the underlying real number. When doing
arithmetics with floating-point numbers, there will be small
approximation errors, called round-off errors or rounding errors,
that may or may not accumulate in comprehensive computations.
A typical example is shown below.

!bc pyshell
>>> 1.0/51*51
1.0
>>> 1.0/49*49
0.9999999999999999
!ec
We see there is not an exact result in the latter case, but a rounding
error of $10^{-16}$. This is the typical level of a rounding error
from an arithmetic operation with 64 bit floating-point numbers
(`float` object in Python, often called `double` or double precision
in other languages).

What is the effect of using `float` objects and not exact arithmetics?
Python has a `Decimal` object in the "`decimal`":
"https://docs.python.org/2/library/decimal.html" module that allows us
to use as many digits in floating-point numbers as we like. We take
1000 digits as the true answer where rounding errors are negligible,
and then we run our numerical algorithm (the Crank-Nicolson scheme to
be precise) with `Decimal` objects for all real numbers and test the
error arising from using 4, 16, 64, and 128 digits.

When computing with numbers around unity in size and doing 40 time
steps, we typically
get a rounding error of $10^{-d}$, where $d$ is the number
of digits used. The effect of rounding errors may accumulate if
we perform more operations, so increasing the number of time steps
to 4000 gives a rounding error of the order $10^{-d+2}$.
Also, if we compute with numbers that
are much larger than unit, we lose accuracy due to rounding
errors. For example, for the $u$ values implied by $I=1000$ and $a=100$,
the rounding errors increase to about $10^{-d+3}$. Below is
a table summarizing a set of experiments. A rough model for
the size of rounding errors is $10^{-d+q+r}$, where $d$ is the
number of digits, we take of the order $10^q$ time steps, and
the size of the numbers in the arithmetic expressions are of
order $10^r$.

|----c-----------------c--------------------------c---------------------c--------------------------------c-------------------|
| digits  | $I=1$, $a=1$; 40 steps | $I=1$, $a=1$; 4000 steps | $I=1000$, $a=100$; 40 steps | $I=10^3$, $a=10^2$; 4000 steps |
|----r-----------------l--------------------------l---------------------l--------------------------------l-------------------|
| 4       | $3.05\cdot 10^{-4}$    | $2.51\cdot 10^{-1}$      | $3.05\cdot 10^{-1}$         | $9.82\cdot 10^{2}$             |
| 16      | $1.71\cdot 10^{-16}$   | $1.42\cdot 10^{-14}$     | $1.58\cdot 10^{-13}$        | $4.84\cdot 10^{-11}$           |
| 64      | $2.99\cdot 10^{-64}$   | $1.80\cdot 10^{-62}$     | $2.06\cdot 10^{-61}$        | $1.04\cdot 10^{-57}$           |
| 128     | $1.60\cdot 10^{-128}$  | $1.56\cdot 10^{-126}$    | $2.41\cdot 10^{-125}$       | $1.07\cdot 10^{-122}$          |
|----------------------------------------------------------------------------------------------------------------------------|

We realize that rounding errors are at the lowest possible level
if we scale the differential equation model,
% if document != "DOCUMENT":
see Section ref{decay:app:scaling},
% endif
the numbers entering the computations are of unity in size,
and if we take a small number of steps (40 steps gives a discretization error
of $5\cdot 10^{-3}$ with the Crank-Nicolson scheme).

The computer code for doing the reported experiments need a new version
of the `solver` function where we do arithmetics with `Decimal`
objects:

@@@CODE src-analysis/errors.py fromto: def solver_decimal@def rounding
The function below carries out the experiments. Note that we can
set the number of digits as we want through the `decimal.getcontext().prec`
variable.

@@@CODE src-analysis/errors.py fromto: def rounding@if __name

===== Discussion of the size of various errors =====
